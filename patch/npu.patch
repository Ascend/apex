diff -Nur '--exclude=.git' apex/.gitignore apex-npu/.gitignore
--- apex/.gitignore	2021-04-12 04:03:22.000000000 +0000
+++ apex-npu/.gitignore	1970-01-01 00:00:00.000000000 +0000
@@ -1,5 +0,0 @@
-apex.egg-info
-dist
-build
-docs/build
-*~
\ No newline at end of file
diff -Nur '--exclude=.git' apex/.gitmodules apex-npu/.gitmodules
--- apex/.gitmodules	2021-04-12 04:03:22.000000000 +0000
+++ apex-npu/.gitmodules	1970-01-01 00:00:00.000000000 +0000
@@ -1,4 +0,0 @@
-[submodule "apex/contrib/csrc/multihead_attn/cutlass"]
-	path = apex/contrib/csrc/multihead_attn/cutlass
-	url = https://github.com/NVIDIA/cutlass.git
-	branch = v1.2.0
diff -Nur '--exclude=.git' apex/apex/amp/_initialize.py apex-npu/apex/amp/_initialize.py
--- apex/apex/amp/_initialize.py	2021-04-12 04:03:22.000000000 +0000
+++ apex-npu/apex/amp/_initialize.py	2021-06-17 07:10:45.373711948 +0000
@@ -1,3 +1,19 @@
+# Copyright (c) 2020, Huawei Technologies.
+# Copyright (c) 2019, NVIDIA CORPORATION.
+# All rights reserved.
+#
+# Licensed under the BSD 3-Clause License  (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+# https://opensource.org/licenses/BSD-3-Clause
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
 import torch
 from torch._six import string_classes
 import functools
@@ -20,9 +36,9 @@
 
 def to_type(dtype, t):
     if isinstance(t, torch.Tensor):
-        if not t.is_cuda:
+        if not 'npu' in t.type():
             # This should not be a hard error, since it may be legitimate.
-            warnings.warn("An input tensor was not cuda.")
+            warnings.warn("An input tensor was not npu.")
         # GANs require this.
         # if t.requires_grad:
         #     warn_or_err("input data requires grad.  Since input data is not a model parameter,\n"
@@ -81,15 +97,15 @@
         for name, param in model.named_parameters():
             if param.is_floating_point():
                 if 'Half' in param.type():
-                    warn_or_err("Found param {} with type {}, expected torch.cuda.FloatTensor.\n"
+                    warn_or_err("Found param {} with type {}, expected torch.npu.FloatTensor.\n"
                         "When using amp.initialize, you do not need to call .half() on your model\n"
                         "before passing it, no matter what optimization level you choose.".format(
                         name, param.type()))
-                elif not param.is_cuda:
-                    warn_or_err("Found param {} with type {}, expected torch.cuda.FloatTensor.\n"
+                elif not 'npu' in param.type():
+                    warn_or_err("Found param {} with type {}, expected torch.npu.FloatTensor.\n"
                         "When using amp.initialize, you need to provide a model with parameters\n"
-                        "located on a CUDA device before passing it no matter what optimization level\n"
-                        "you chose. Use model.to('cuda') to use the default device.".format(
+                        "located on a Npu device before passing it no matter what optimization level\n"
+                        "you chose. Use model.to('npu') to use the default device.".format(
                         name, param.type()))
 
         # Backward compatibility for PyTorch 0.4
@@ -104,15 +120,15 @@
                 name, buf = obj, buf_iter[obj]
             if buf.is_floating_point():
                 if 'Half' in buf.type():
-                    warn_or_err("Found buffer {} with type {}, expected torch.cuda.FloatTensor.\n"
+                    warn_or_err("Found buffer {} with type {}, expected torch.npu.FloatTensor.\n"
                         "When using amp.initialize, you do not need to call .half() on your model\n"
                         "before passing it, no matter what optimization level you choose.".format(
                         name, buf.type()))
-                elif not buf.is_cuda:
-                    warn_or_err("Found buffer {} with type {}, expected torch.cuda.FloatTensor.\n"
+                elif not 'npu' in buf.type():
+                    warn_or_err("Found buffer {} with type {}, expected torch.npu.FloatTensor.\n"
                         "When using amp.initialize, you need to provide a model with buffers\n"
-                        "located on a CUDA device before passing it no matter what optimization level\n"
-                        "you chose. Use model.to('cuda') to use the default device.".format(
+                        "located on a Npu device before passing it no matter what optimization level\n"
+                        "you chose. Use model.to('npu') to use the default device.".format(
                         name, buf.type()))
 
 
diff -Nur '--exclude=.git' apex/apex/amp/_process_optimizer.py apex-npu/apex/amp/_process_optimizer.py
--- apex/apex/amp/_process_optimizer.py	2021-04-12 04:03:22.000000000 +0000
+++ apex-npu/apex/amp/_process_optimizer.py	2021-06-17 07:10:45.373711948 +0000
@@ -1,9 +1,52 @@
+# Copyright (c) 2020, Huawei Technologies.
+# Copyright (c) 2019, NVIDIA CORPORATION.
+# All rights reserved.
+#
+# Licensed under the BSD 3-Clause License  (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+# https://opensource.org/licenses/BSD-3-Clause
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
 import types
 from ..fp16_utils import master_params_to_model_params
 from ..multi_tensor_apply import multi_tensor_applier
 from ._amp_state import maybe_print
 import torch
 from ..optimizers import FusedSGD
+from ..contrib.combine_tensors import combine_npu, get_part_combined_tensor, is_combined_tensor_valid
+
+
+def get_grad_combined_tensor_from_param(list_of_params):
+    if len(list_of_params) > 0 and list_of_params[0].grad is not None:
+        list_of_grad = []
+        for param in list_of_params:
+            if param.requires_grad:
+                list_of_grad.append(param.grad)
+        original_combined_tensor = combine_npu(list_of_grad)
+        return original_combined_tensor, list_of_grad
+    else:
+        return None, []
+
+
+def get_grad_combined_tensor_mask_from_param(list_of_params):
+    if len(list_of_params) > 0 and list_of_params[0].grad is not None:
+        list_of_grad_mask = []
+        for param in list_of_params:
+            if param.requires_grad:
+                grad_size = param.grad.size()
+                grad_format = param.storage().npu_format()
+                list_of_grad_mask.append(torch.ones(grad_size).npu().npu_format_cast(grad_format))
+        grad_combined_tensor_mask = combine_npu(list_of_grad_mask)
+        return grad_combined_tensor_mask
+    else:
+        return None
 
 
 class AmpOptimizerState(object):
@@ -26,96 +69,114 @@
 
 
 def lazy_init_with_master_weights(self):
-        stash = self._amp_stash
-        stash.fp16_groups = []
-        stash.fp32_from_fp16_groups = []
-        stash.fp32_from_fp32_groups = []
-        for i, param_group in enumerate(self.param_groups):
-            # maybe_print("FP16_Optimizer processing param group {}:".format(i))
-            fp16_params_this_group = []
-            fp32_params_this_group = []
-            fp32_from_fp16_params_this_group = []
-            for i, param in enumerate(param_group['params']):
-                if param.requires_grad:
-                    if param.type() == 'torch.cuda.HalfTensor':
-                        # maybe_print("FP16_Optimizer received torch.cuda.HalfTensor with {}"
-                        #             .format(param.size()))
-                        fp16_params_this_group.append(param)
-                        master_param = param.detach().clone().float()
-                        master_param.requires_grad = True
-                        param_group['params'][i] = master_param
-                        fp32_from_fp16_params_this_group.append(master_param)
-                        # Reset existing state dict key to the new master param.
-                        # We still need to recast per-param state tensors, if any, to FP32.
-                        if param in self.state:
-                           self.state[master_param] = self.state.pop(param)
-                    elif param.type() == 'torch.cuda.FloatTensor':
-                        # maybe_print("FP16_Optimizer received torch.cuda.FloatTensor with {}"
-                        #             .format(param.size()))
-                        fp32_params_this_group.append(param)
-                        param_group['params'][i] = param
-                    else:
-                        raise TypeError("Optimizer's parameters must be either "
-                                        "torch.cuda.FloatTensor or torch.cuda.HalfTensor. "
-                                        "Received {}".format(param.type()))
-
-            stash.fp16_groups.append(fp16_params_this_group)
-            stash.fp32_from_fp16_groups.append(fp32_from_fp16_params_this_group)
-            stash.fp32_from_fp32_groups.append(fp32_params_this_group)
+    stash = self._amp_stash
+    stash.fp16_groups = []
+    stash.fp32_from_fp16_groups = []
+    stash.fp32_from_fp32_groups = []
+    for i, param_group in enumerate(self.param_groups):
+        # maybe_print("FP16_Optimizer processing param group {}:".format(i))
+        fp16_params_this_group = []
+        fp32_params_this_group = []
+        fp32_from_fp16_params_this_group = []
+        for i, param in enumerate(param_group['params']):
+            if param.requires_grad:
+                if param.type() == 'torch.npu.HalfTensor':
+                    # maybe_print("FP16_Optimizer received torch.cuda.HalfTensor with {}"
+                    #             .format(param.size()))
+                    fp16_params_this_group.append(param)
+                    master_param = param.detach().clone().float()
+                    master_param.requires_grad = True
+                    param_group['params'][i] = master_param
+                    fp32_from_fp16_params_this_group.append(master_param)
+                    # Reset existing state dict key to the new master param.
+                    # We still need to recast per-param state tensors, if any, to FP32.
+                    if param in self.state:
+                        self.state[master_param] = self.state.pop(param)
+                elif param.type() == 'torch.npu.FloatTensor':
+                    # maybe_print("FP16_Optimizer received torch.cuda.FloatTensor with {}"
+                    #             .format(param.size()))
+                    fp32_params_this_group.append(param)
+                    param_group['params'][i] = param
+                else:
+                    raise TypeError("Optimizer's parameters must be either "
+                                    "torch.cuda.FloatTensor or torch.cuda.HalfTensor. "
+                                    "Received {}".format(param.type()))
+
+        stash.fp16_groups.append(fp16_params_this_group)
+        stash.fp32_from_fp16_groups.append(fp32_from_fp16_params_this_group)
+        stash.fp32_from_fp32_groups.append(fp32_params_this_group)
 
-        stash.all_fp16_params = []
-        for group in stash.fp16_groups:
-            stash.all_fp16_params += group
-
-        stash.all_fp32_from_fp16_params = []
-        for group in stash.fp32_from_fp16_groups:
-            stash.all_fp32_from_fp16_params += group
-
-        stash.all_fp32_from_fp32_params = []
-        for group in stash.fp32_from_fp32_groups:
-            stash.all_fp32_from_fp32_params += group
-
-        # all_fp16_grad_stash is only needed for fused optimizers.
-        stash.all_fp16_grad_stash = [None for _ in stash.all_fp16_params]
-        # stash.all_fp32_from_fp16_grad_stash = [None for _ in stash.all_fp32_from_fp16_params]
-        stash.all_fp32_from_fp32_grad_stash = [None for _ in stash.all_fp32_from_fp32_params]
+    stash.all_fp16_params = []
+    for group in stash.fp16_groups:
+        stash.all_fp16_params += group
 
-        for param in stash.all_fp32_from_fp16_params:
-            param.grad = None
+    stash.all_fp32_from_fp16_params = []
+    for group in stash.fp32_from_fp16_groups:
+        stash.all_fp32_from_fp16_params += group
+
+    stash.all_fp32_from_fp32_params = []
+    for group in stash.fp32_from_fp32_groups:
+        stash.all_fp32_from_fp32_params += group
 
-        for param in stash.all_fp32_from_fp32_params:
-            param.grad = None
+    # all_fp16_grad_stash is only needed for fused optimizers.
+    stash.all_fp16_grad_stash = [None for _ in stash.all_fp16_params]
+    # stash.all_fp32_from_fp16_grad_stash = [None for _ in stash.all_fp32_from_fp16_params]
+    stash.all_fp32_from_fp32_grad_stash = [None for _ in stash.all_fp32_from_fp32_params]
 
-        # Leverage state_dict() and load_state_dict() to recast preexisting per-param state tensors
-        self.load_state_dict(self.state_dict())
+    for param in stash.all_fp32_from_fp16_params:
+        param.grad = None
 
+    for param in stash.all_fp32_from_fp32_params:
+        param.grad = None
+    
+    stash.main_fp16_grad_combine = None
+    stash.main_fp32_from_fp16_grad_combine = None
+    stash.main_fp32_from_fp32_grad_combine = None
+    stash.main_fp32_from_fp16_grad_combine_mask = None
+    stash.main_fp32_from_fp32_grad_combine_mask = None
+
+    stash.all_fp32_from_fp32_grad_stash_combine = None
+
+    stash.main_fp16_param_combine = None
+    stash.main_fp32_from_fp16_param_combine = None
+    stash.main_fp32_from_fp32_param_combine = None
+    # Leverage state_dict() and load_state_dict() to recast preexisting per-param state tensors
+    self.load_state_dict(self.state_dict())
+
+
+def post_backward_models_are_masters(scaler, params, stashed_grads, scale_override=None, 
+                                     main_grads_combined=None, stashed_grads_combined=None, 
+                                     use_npu_fused_optimizer=False, stashed_grads_are_zero=False):
+    grads_have_scale, stashed_have_scale, out_scale = scaler.loss_scale(), 1.0, 1.0
 
-def post_backward_models_are_masters(scaler, params, stashed_grads, scale_override=None):
-        grads_have_scale, stashed_have_scale, out_scale = scaler.loss_scale(), 1.0, 1.0
+    # not much to do if scale == 1.0 and static scaling
+    if scaler.loss_scale() == 1.0 and not scaler.dynamic:
+        # Clear the stash.
+        for i in range(len(stashed_grads)):
+            stashed_grads[i] = None
+        return
 
-        # not much to do if scale == 1.0 and static scaling
-        if scaler.loss_scale() == 1.0 and not scaler.dynamic:
-            # Clear the stash.
-            for i in range(len(stashed_grads)):
-                stashed_grads[i] = None
-            return
-        
-        if scale_override is not None:
-            grads_have_scale, stashed_have_scale, out_scale = scale_override
+    if scale_override is not None:
+        grads_have_scale, stashed_have_scale, out_scale = scale_override
 
-        # This is a lot of python overhead...
+    # This is a lot of python overhead...
+    if main_grads_combined is not None:
+        scaler.unscale_with_stashed_combined(
+            main_grads_combined, stashed_grads_combined,
+            scale_override=(grads_have_scale, stashed_have_scale, out_scale))
+    else:
         grads_needing_unscale = []
         grads_needing_unscale_with_stash = []
         stashed = []
         for param, stashed_grad in zip(params, stashed_grads):
             if param.grad is None and stashed_grad is not None:
                 param.grad = stashed_grad
-            elif param.grad is not None and stashed_grad is None:
+            elif param.grad is not None and (stashed_grad is None or stashed_grads_are_zero):
                 grads_needing_unscale.append(param.grad)
             elif param.grad is not None and stashed_grad is not None:
                 grads_needing_unscale_with_stash.append(param.grad)
                 stashed.append(stashed_grad)
-            else: # param.grad is None and stashed_grad is None
+            else:  # param.grad is None and stashed_grad is None
                 continue
 
         # unscale() implements grads*(1/scale), so "scale" should be grads_have_scale/out_scale.
@@ -123,130 +184,259 @@
             scaler.unscale(
                 grads_needing_unscale,
                 grads_needing_unscale,
-                None, # unused_scale, currently present to avoid API breakage elsewhere
+                None,  # unused_scale, currently present to avoid API breakage elsewhere
                 models_are_masters=True,
-                scale_override=grads_have_scale/out_scale)
+                scale_override=grads_have_scale / out_scale)
 
         if len(grads_needing_unscale_with_stash) > 0:
             scaler.unscale_with_stashed(
                 grads_needing_unscale_with_stash,
                 stashed,
                 grads_needing_unscale_with_stash,
-                scale_override=(grads_have_scale, stashed_have_scale, out_scale))
+                scale_override=(grads_have_scale, stashed_have_scale, out_scale),
+                use_npu_fused_optimizer=use_npu_fused_optimizer)
 
-        # Clear the stash.
-        for i in range(len(stashed_grads)):
-            stashed_grads[i] = None
+        if not use_npu_fused_optimizer:
+            # Clear the stash.
+            for i in range(len(stashed_grads)):
+                stashed_grads[i] = None
 
 
 def prepare_backward_with_master_weights(self):
     stash = self._amp_stash
 
     self._amp_lazy_init()
+    self._check_already_combined_params_and_grads()
 
-    for i, param in enumerate(stash.all_fp16_params):
-        # Set up to leverage grad copy elision.
-        # This may behave differently from an unpatched optimizer if zero_grad is used and the param is unused.
-        param.grad = None
+    if (self.accelerate or self.is_npu_fused_optimizer) and stash.already_combined:
+        if stash.process_zero_grad:
+            return
+
+        if stash.main_fp16_grad_combine is not None:
+            stash.main_fp16_grad_combine.zero_()
+
+        if stash.main_fp32_from_fp32_grad_combine is not None:
+            stash.all_fp32_from_fp32_grad_stash_combine.copy_(stash.main_fp32_from_fp32_grad_combine)
+            stash.main_fp32_from_fp32_grad_combine.zero_()
+    else:
+        for i, param in enumerate(stash.all_fp16_params):
+            # Set up to leverage grad copy elision.
+            # This may behave differently from an unpatched optimizer if zero_grad is used and the param is unused.
+            param.grad = None
+
+        # for i, param in enumerate(stash.all_fp32_from_fp16_params):
+        #     stash.all_fp32_from_fp16_grad_stash[i] = param.grad
+
+        for i, param in enumerate(stash.all_fp32_from_fp32_params):
+            stash.all_fp32_from_fp32_grad_stash[i] = param.grad
+            # Set up to leverage grad copy elision:
+            param.grad = None
+
+
+@torch.no_grad()
+def combined_init_with_master_weights(self):
+    stash = self._amp_stash
+    if stash.already_combined:
+        return
 
-    # for i, param in enumerate(stash.all_fp32_from_fp16_params):
-    #     stash.all_fp32_from_fp16_grad_stash[i] = param.grad
+    if (not self.accelerate) and (not self.is_npu_fused_optimizer):
+        return
 
     for i, param in enumerate(stash.all_fp32_from_fp32_params):
-        stash.all_fp32_from_fp32_grad_stash[i] = param.grad
-        # Set up to leverage grad copy elision:
-        param.grad = None
+        if param.grad is not None:
+            stash.all_fp32_from_fp32_grad_stash[i] = torch.zeros_like(param.grad)
+
+    if len(stash.all_fp32_from_fp32_grad_stash) > 0:
+        stash.all_fp32_from_fp32_grad_stash_combine = combine_npu(stash.all_fp32_from_fp32_grad_stash)
+
+    all_fp16_params, all_fp32_from_fp16_params = [], []
+    for fp16_param, fp32_from_fp16_param in zip(stash.all_fp16_params, stash.all_fp32_from_fp16_params):
+        if fp16_param.grad is not None:
+            all_fp16_params.append(fp16_param)
+            all_fp32_from_fp16_params.append(fp32_from_fp16_param)
+    stash.all_fp16_params = all_fp16_params
+    stash.all_fp32_from_fp16_params = all_fp32_from_fp16_params
+
+    stash.main_fp16_grad_combine, stash.fp16_grad_list = get_grad_combined_tensor_from_param(stash.all_fp16_params)
+
+    for fp16_grad, fp32_from_fp16_param in zip(stash.fp16_grad_list, stash.all_fp32_from_fp16_params):
+        if fp16_grad.storage().npu_format() == fp32_from_fp16_param.storage().npu_format():
+            fp32_from_fp16_param.grad = torch.zeros_like(fp32_from_fp16_param)
+        else:
+            fp32_from_fp16_param.grad = torch.zeros_like(fp16_grad.to(torch.float))
+            fp32_from_fp16_param.data = fp32_from_fp16_param.data.npu_format_cast(fp16_grad.storage().npu_format())
+
+    stash.main_fp32_from_fp16_grad_combine, stash.fp32_from_fp16_grad_list = \
+        get_grad_combined_tensor_from_param(stash.all_fp32_from_fp16_params)
+    stash.main_fp32_from_fp32_grad_combine, stash.fp32_from_fp32_grad_list = \
+        get_grad_combined_tensor_from_param(stash.all_fp32_from_fp32_params)
+    # please do not change the order of tensor in this list.
+    stash.grads_list = [stash.main_fp16_grad_combine, 
+                        stash.main_fp32_from_fp16_grad_combine, 
+                        stash.main_fp32_from_fp32_grad_combine]
+
+    if self.is_npu_fused_optimizer:
+        # stash.main_fp16_param_combine = combine_npu(stash.all_fp16_params)
+        stash.main_fp32_from_fp16_param_combine = combine_npu(stash.all_fp32_from_fp16_params)
+        stash.main_fp32_from_fp32_param_combine = combine_npu(stash.all_fp32_from_fp32_params)
+    
+    stash.already_combined = True
 
 
 def post_backward_with_master_weights(self, scaler):
     stash = self._amp_stash
 
     self._amp_lazy_init()
+    self._check_already_combined_params_and_grads()
+    self._amp_combined_init()
 
-    # This is a lot of python overhead...
-    fp16_grads_needing_unscale = []
-    new_fp32_grads = []
-    fp16_grads_needing_unscale_with_stash = []
-    preexisting_fp32_grads = []
-    for fp16_param, fp32_param in zip(stash.all_fp16_params,
-                                      stash.all_fp32_from_fp16_params):
-        if fp16_param.grad is None and fp32_param.grad is not None:
-            continue
-        elif fp16_param.grad is not None and fp32_param.grad is None:
-            fp32_param.grad = torch.empty_like(fp32_param)
-            fp16_grads_needing_unscale.append(fp16_param.grad)
-            new_fp32_grads.append(fp32_param.grad)
-        elif fp16_param.grad is not None and fp32_param.grad is not None:
-            fp16_grads_needing_unscale_with_stash.append(fp16_param.grad)
-            preexisting_fp32_grads.append(fp32_param.grad)
-        else: # fp16_param.grad is None and fp32_param.grad is None:
-            continue
-
-    if len(fp16_grads_needing_unscale) > 0:
-        scaler.unscale(
-            fp16_grads_needing_unscale,
-            new_fp32_grads,
-            scaler.loss_scale(),
-            models_are_masters=False)
-
-    if len(fp16_grads_needing_unscale_with_stash) > 0:
-        scaler.unscale_with_stashed(
-            fp16_grads_needing_unscale_with_stash,
-            preexisting_fp32_grads,
-            preexisting_fp32_grads)
-
-    # fp32 params can be treated as they would be in the "no_master_weights" case.
-    post_backward_models_are_masters(
-        scaler,
-        stash.all_fp32_from_fp32_params,
-        stash.all_fp32_from_fp32_grad_stash)
+    if self.accelerate:
+        scaler.unscale_grad_O2(
+            model_grads_combined=stash.main_fp16_grad_combine,
+            stashed_master_grads_combined=stash.main_fp32_from_fp16_grad_combine if not stash.process_zero_grad else None,
+            master_grads_combined=stash.main_fp32_from_fp16_grad_combine,
+            master_grads=stash.fp32_from_fp16_grad_list,
+            model_grads=stash.fp16_grad_list)
+        if stash.main_fp32_from_fp32_grad_combine is not None:
+            scaler.unscale_grad_O2(
+                model_grads_combined=stash.main_fp32_from_fp32_grad_combine,
+                stashed_master_grads_combined=stash.all_fp32_from_fp32_grad_stash_combine if not stash.process_zero_grad else None,
+                master_grads_combined=stash.main_fp32_from_fp32_grad_combine)
+    else:
+        # This is a lot of python overhead...
+        fp16_grads_needing_unscale = []
+        new_fp32_grads = []
+        fp16_grads_needing_unscale_with_stash = []
+        preexisting_fp32_grads = []
+        for fp16_param, fp32_param in zip(stash.all_fp16_params,
+                                          stash.all_fp32_from_fp16_params):
+            if fp16_param.grad is None and fp32_param.grad is not None:
+                continue
+            elif fp16_param.grad is not None and fp32_param.grad is None:
+                fp32_param.grad = torch.empty_like(fp32_param)
+                fp16_grads_needing_unscale.append(fp16_param.grad)
+                new_fp32_grads.append(fp32_param.grad)
+            elif fp16_param.grad is not None and fp32_param.grad is not None:
+                if stash.process_zero_grad:
+                    fp16_grads_needing_unscale.append(fp16_param.grad)
+                    new_fp32_grads.append(fp32_param.grad)
+                else:
+                    fp16_grads_needing_unscale_with_stash.append(fp16_param.grad)
+                    preexisting_fp32_grads.append(fp32_param.grad)
+            else: # fp16_param.grad is None and fp32_param.grad is None:
+                continue
+
+        if len(fp16_grads_needing_unscale) > 0:
+            scaler.unscale(
+                fp16_grads_needing_unscale,
+                new_fp32_grads,
+                scaler.loss_scale(),
+                models_are_masters=False)
+
+        if len(fp16_grads_needing_unscale_with_stash) > 0:
+            scaler.unscale_with_stashed(
+                fp16_grads_needing_unscale_with_stash,
+                preexisting_fp32_grads,
+                preexisting_fp32_grads,
+                use_npu_fused_optimizer=self.is_npu_fused_optimizer)
+
+        # fp32 params can be treated as they would be in the "no_master_weights" case.
+        post_backward_models_are_masters(
+            scaler,
+            stash.all_fp32_from_fp32_params,
+            stash.all_fp32_from_fp32_grad_stash,
+            use_npu_fused_optimizer=self.is_npu_fused_optimizer,
+            stashed_grads_are_zero=stash.process_zero_grad)
+    
+    stash.process_zero_grad = False
 
 
 def lazy_init_no_master_weights(self):
     stash = self._amp_stash
     stash.all_fp16_params = []
     stash.all_fp32_params = []
+
+    check_param_require_grad = self.accelerate or self.is_npu_fused_optimizer
+
     for i, param_group in enumerate(self.param_groups):
         for i, param in enumerate(param_group['params']):
-            if param.type() == 'torch.cuda.HalfTensor':
+            if check_param_require_grad and not param.requires_grad:
+                continue
+
+            if param.type() == 'torch.npu.HalfTensor':
                 stash.all_fp16_params.append(param)
-            elif param.type() == 'torch.cuda.FloatTensor':
+            elif param.type() == 'torch.npu.FloatTensor':
                 stash.all_fp32_params.append(param)
             else:
                 raise TypeError("Optimizer's parameters must be either "
-                                "torch.cuda.FloatTensor or torch.cuda.HalfTensor. "
+                                "torch.npu.FloatTensor or torch.npu.HalfTensor."
                                 "Received {}".format(param.type()))
 
     stash.all_fp16_grad_stash = [None for _ in stash.all_fp16_params]
     stash.all_fp32_grad_stash = [None for _ in stash.all_fp32_params]
 
+    stash.all_fp16_grad_stash_combine = None
+    stash.all_fp32_grad_stash_combine = None
+
+    stash.fp16_grad_list = []
+    stash.main_fp16_grad_combine = None
+    stash.main_fp16_grad_combine_mask = None
+
+    stash.fp32_grad_list = []
+    stash.main_fp32_grad_combine = None
+    stash.main_fp32_grad_combine_mask = None
+
+    stash.main_fp16_param_combine = None
+    stash.main_fp32_param_combine = None
+
 
 def prepare_backward_no_master_weights(self):
     stash = self._amp_stash
 
     self._amp_lazy_init()
+    self._check_already_combined_params_and_grads()
 
-    for i, param in enumerate(stash.all_fp16_params):
-        stash.all_fp16_grad_stash[i] = param.grad
-        # Set up to leverage grad copy elision:
-        param.grad = None
+    if (self.accelerate or self.is_npu_fused_optimizer) and stash.already_combined:
+        if stash.main_fp16_grad_combine is not None:
+            stash.all_fp16_grad_stash_combine.copy_(stash.main_fp16_grad_combine)
+            stash.main_fp16_grad_combine.zero_()
+        if stash.main_fp32_grad_combine is not None:
+            stash.all_fp32_grad_stash_combine.copy_(stash.main_fp32_grad_combine)
+            stash.main_fp32_grad_combine.zero_()
+    else:
+        for i, param in enumerate(stash.all_fp16_params):
+            stash.all_fp16_grad_stash[i] = param.grad
+            # Set up to leverage grad copy elision:
+            param.grad = None
 
-    for i, param in enumerate(stash.all_fp32_params):
-        stash.all_fp32_grad_stash[i] = param.grad
-        # Set up to leverage grad copy elision:
-        param.grad = None
+        for i, param in enumerate(stash.all_fp32_params):
+            stash.all_fp32_grad_stash[i] = param.grad
+            # Set up to leverage grad copy elision:
+            param.grad = None
 
 
 def post_backward_no_master_weights(self, scaler):
     stash = self._amp_stash
 
     self._amp_lazy_init()
+    self._check_already_combined_params_and_grads()
+    self._amp_combined_init()
 
-    split_types = ((stash.all_fp16_params, stash.all_fp16_grad_stash),
-             (stash.all_fp32_params, stash.all_fp32_grad_stash))
+    if self.accelerate:
+        split_types = ((stash.main_fp16_grad_combine, stash.all_fp16_grad_stash_combine),
+                (stash.main_fp32_grad_combine, stash.all_fp32_grad_stash_combine))
+        for main_grads_combined, stash_grads_combined  in split_types:
+            if main_grads_combined is not None:
+                post_backward_models_are_masters(scaler, None, None, None, 
+                                                 main_grads_combined, stash_grads_combined,
+                                                 use_npu_fused_optimizer=self.is_npu_fused_optimizer)
+    else:
+        split_types = ((stash.all_fp16_params, stash.all_fp16_grad_stash),
+                 (stash.all_fp32_params, stash.all_fp32_grad_stash))
 
-    for params, stashed_grads in split_types:
-        post_backward_models_are_masters(scaler, params, stashed_grads)
+        for params, stashed_grads in split_types:
+            post_backward_models_are_masters(scaler, params, stashed_grads, 
+                                             use_npu_fused_optimizer=self.is_npu_fused_optimizer)
 
 
 #####################################################################################
@@ -318,6 +508,420 @@
         stash.lazy_init_called = True
 
 
+@torch.no_grad()
+def combined_init_no_master_weights(self):
+    stash = self._amp_stash
+    if stash.already_combined:
+        return
+
+    if (not self.accelerate) and (not self.is_npu_fused_optimizer):
+        return
+
+    all_fp16_params, all_fp16_grad_stash = [], []
+    for param in stash.all_fp16_params:
+        if param.grad is not None:
+            all_fp16_params.append(param)
+            all_fp16_grad_stash.append(torch.zeros_like(param.grad))
+
+    stash.all_fp16_params = all_fp16_params
+    stash.all_fp16_grad_stash = all_fp16_grad_stash
+
+    all_fp32_params, all_fp32_grad_stash = [], []
+    for param in stash.all_fp32_params:
+        if param.grad is not None:
+            all_fp32_params.append(param)
+            all_fp32_grad_stash.append(torch.zeros_like(param.grad))
+
+    stash.all_fp32_params = all_fp32_params
+    stash.all_fp32_grad_stash = all_fp32_grad_stash
+
+    if len(stash.all_fp16_grad_stash) > 0:
+        # if len == 0, avoid to create a useless combined tensor
+        stash.all_fp16_grad_stash_combine = combine_npu(stash.all_fp16_grad_stash, require_copy_value=False)
+    if len(stash.all_fp32_grad_stash) > 0:
+        stash.all_fp32_grad_stash_combine = combine_npu(stash.all_fp32_grad_stash, require_copy_value=False)
+
+    stash.main_fp16_grad_combine, stash.fp16_grad_list = get_grad_combined_tensor_from_param(stash.all_fp16_params)
+    stash.main_fp32_grad_combine, stash.fp32_grad_list = get_grad_combined_tensor_from_param(stash.all_fp32_params)
+    # please do not change the order of tensor in this list.
+    stash.grads_list = [stash.main_fp16_grad_combine, stash.main_fp32_grad_combine]
+
+    if self.is_npu_fused_optimizer:
+        # stash.main_fp16_param_combine = combine_npu(stash.all_fp16_params)
+        stash.main_fp32_param_combine = combine_npu(stash.all_fp32_params)
+
+    stash.already_combined = True
+
+
+def reset_all_combine_flags(self):
+    stash = self._amp_stash
+    stash.already_combined = False
+    stash.params_grads_are_combined_by_group = False
+    stash.param_states_are_combined_by_group = False
+
+
+def check_already_combined_params_and_grads_no_master_weights(self):
+    stash = self._amp_stash
+    if not self.check_combined_tensors or not stash.already_combined:
+        return
+
+    if not is_combined_tensor_valid(stash.main_fp16_grad_combine, stash.fp16_grad_list) or \
+        not is_combined_tensor_valid(stash.main_fp32_grad_combine, stash.fp32_grad_list):
+        maybe_print("Combined grad has been destroyed and will be recombined afterwards, please check if "
+                    "there is any operation that may change the data_ptr/size/format of the grads.")
+        self._reset_all_combine_flags()
+        return
+
+    if self.is_npu_fused_optimizer:
+        if not is_combined_tensor_valid(stash.main_fp32_param_combine, stash.all_fp32_params):
+            maybe_print("Combined param has been destroyed and will be recombined afterwards, please check if "
+                        "there is any operation that may change the data_ptr/size/format of the params.")
+            self._reset_all_combine_flags()
+            return
+
+
+def check_already_combined_params_and_grads_with_master_weights(self):
+    stash = self._amp_stash
+    if not self.check_combined_tensors or not stash.already_combined:
+        return
+
+    if not is_combined_tensor_valid(stash.main_fp16_grad_combine, stash.fp16_grad_list) or \
+        not is_combined_tensor_valid(stash.main_fp32_from_fp32_grad_combine, stash.fp32_from_fp32_grad_list):
+        maybe_print("Combined grad has been destroyed and will be recombined afterwards, please check if "
+                    "there is any operation that may change the data_ptr/size/format of the grads.")
+        self._reset_all_combine_flags()
+        return
+
+    if self.is_npu_fused_optimizer:
+        if not is_combined_tensor_valid(stash.main_fp32_from_fp32_param_combine, stash.all_fp32_from_fp32_params):
+            maybe_print("Combined param has been destroyed and will be recombined afterwards, please check if "
+                        "there is any operation that may change the data_ptr/size/format of the params.")
+            self._reset_all_combine_flags()
+            return
+
+
+def is_grad_in_combined_tensor(grad, combined_tensor):
+    if combined_tensor is None:
+        return False
+
+    combined_tensor_data_start_addr = combined_tensor.data_ptr()
+    combined_tensor_data_end_addr = combined_tensor.data_ptr() + \
+                                    combined_tensor.numel() * combined_tensor.element_size()
+    
+    if combined_tensor_data_start_addr <= grad.data_ptr() < combined_tensor_data_end_addr:
+        return True
+    else:
+        return False
+
+
+def combine_params_and_grads_by_group_no_master_weights(self):
+    stash = self._amp_stash
+    if stash.params_grads_are_combined_by_group:
+        return
+
+    self._amp_combined_init()
+    stash.combined_params_indexed_by_group = []
+    stash.combined_grads_indexed_by_group = []
+    stash.params_lists_indexed_by_group = []
+
+    combined_fp32_param = stash.main_fp32_param_combine
+    combined_fp32_grad = stash.main_fp32_grad_combine
+
+    combined_group_fp32_param_index = 0
+    combined_group_fp32_grad_index = 0
+
+    group_num = 0
+    for group in self.param_groups:
+        group_num += 1
+
+        group_fp32_params = []
+        group_fp32_param_size = 0
+        group_fp32_grad_size = 0
+
+        for p in group['params']:
+            if p.grad is None:
+                continue
+
+            param_size = p.storage().size()
+            group_fp32_param_size += param_size
+            group_fp32_params.append(p)
+
+            grad_size = p.grad.storage().size()
+            group_fp32_grad_size += grad_size
+
+        combined_group_fp32_param = None
+        combined_group_fp32_grad = None
+        combined_group_fp32_param = get_part_combined_tensor(combined_fp32_param, 
+                                                             combined_group_fp32_param_index,
+                                                             group_fp32_param_size)
+        combined_group_fp32_grad = get_part_combined_tensor(combined_fp32_grad, 
+                                                            combined_group_fp32_grad_index, 
+                                                            group_fp32_grad_size)
+        combined_group_fp32_param_index += group_fp32_param_size
+        combined_group_fp32_grad_index += group_fp32_grad_size
+
+        combined_params = []
+        combined_grads = []
+        params_list = []
+
+        combined_params.append(combined_group_fp32_param)
+        combined_grads.append(combined_group_fp32_grad)
+        params_list.append(group_fp32_params)
+
+        stash.combined_params_indexed_by_group.append(combined_params)
+        stash.combined_grads_indexed_by_group.append(combined_grads)
+        stash.params_lists_indexed_by_group.append(params_list)
+
+    maybe_print("group num: {}".format(group_num))
+    stash.params_grads_are_combined_by_group = True
+
+
+def combine_params_and_grads_by_group_with_master_weights(self):
+    stash = self._amp_stash
+    if stash.params_grads_are_combined_by_group:
+        return
+
+    self._amp_combined_init()
+    stash.combined_params_indexed_by_group = []
+    stash.combined_grads_indexed_by_group = []
+    stash.params_lists_indexed_by_group = []
+
+    combined_fp32_from_fp32_param = stash.main_fp32_from_fp32_param_combine
+    combined_fp32_from_fp16_param = stash.main_fp32_from_fp16_param_combine
+    combined_fp32_from_fp32_grad = stash.main_fp32_from_fp32_grad_combine
+    combined_fp32_from_fp16_grad = stash.main_fp32_from_fp16_grad_combine
+
+    combined_group_fp32_from_fp32_param_index, combined_group_fp32_from_fp16_param_index = 0, 0
+    combined_group_fp32_from_fp32_grad_index, combined_group_fp32_from_fp16_grad_index = 0, 0
+
+    group_num = 0
+    for group in self.param_groups:
+        group_num += 1
+
+        group_fp32_from_fp32_params = []
+        group_fp32_from_fp16_params = []
+        group_fp32_from_fp32_param_size, group_fp32_from_fp16_param_size = 0, 0
+        group_fp32_from_fp32_grad_size, group_fp32_from_fp16_grad_size = 0, 0
+
+        for p in group['params']:
+            if p.grad is None:
+                continue
+
+            param_size = p.storage().size()
+            grad_size = p.grad.storage().size()
+            if is_grad_in_combined_tensor(p.grad, combined_fp32_from_fp32_grad):
+                group_fp32_from_fp32_param_size += param_size
+                group_fp32_from_fp32_params.append(p)
+                group_fp32_from_fp32_grad_size += grad_size
+            else:
+                group_fp32_from_fp16_param_size += param_size
+                group_fp32_from_fp16_params.append(p)
+                group_fp32_from_fp16_grad_size += grad_size
+
+        combined_group_fp32_from_fp32_param = None
+        combined_group_fp32_from_fp16_param = None
+        combined_group_fp32_from_fp32_grad = None
+        combined_group_fp32_from_fp16_grad = None
+
+        combined_group_fp32_from_fp32_param = get_part_combined_tensor(combined_fp32_from_fp32_param,
+                                                                       combined_group_fp32_from_fp32_param_index,
+                                                                       group_fp32_from_fp32_param_size)
+        combined_group_fp32_from_fp16_param = get_part_combined_tensor(combined_fp32_from_fp16_param,
+                                                                       combined_group_fp32_from_fp16_param_index,
+                                                                       group_fp32_from_fp16_param_size)
+        combined_group_fp32_from_fp32_grad = get_part_combined_tensor(combined_fp32_from_fp32_grad, 
+                                                                      combined_group_fp32_from_fp32_grad_index,
+                                                                      group_fp32_from_fp32_grad_size)
+        combined_group_fp32_from_fp16_grad = get_part_combined_tensor(combined_fp32_from_fp16_grad, 
+                                                                      combined_group_fp32_from_fp16_grad_index,
+                                                                      group_fp32_from_fp16_grad_size)
+
+        combined_group_fp32_from_fp32_param_index += group_fp32_from_fp32_param_size
+        combined_group_fp32_from_fp16_param_index += group_fp32_from_fp16_param_size
+        combined_group_fp32_from_fp32_grad_index += group_fp32_from_fp32_grad_size
+        combined_group_fp32_from_fp16_grad_index += group_fp32_from_fp16_grad_size
+
+        combined_params = []
+        combined_grads = []
+        params_list = []
+
+        combined_params.append(combined_group_fp32_from_fp32_param)
+        combined_params.append(combined_group_fp32_from_fp16_param)
+        combined_grads.append(combined_group_fp32_from_fp32_grad)
+        combined_grads.append(combined_group_fp32_from_fp16_grad)
+        params_list.append(group_fp32_from_fp32_params)
+        params_list.append(group_fp32_from_fp16_params)
+
+        stash.combined_params_indexed_by_group.append(combined_params)
+        stash.combined_grads_indexed_by_group.append(combined_grads)
+        stash.params_lists_indexed_by_group.append(params_list)
+
+    maybe_print("group num: {}".format(group_num))
+    stash.params_grads_are_combined_by_group = True
+
+
+def new_zero_grad_with_master_weights(self):
+    stash = self._amp_stash
+    self._amp_lazy_init()
+    # Zero the model grads.
+    for param in stash.all_fp16_params:
+        if param.grad is not None:
+            param.grad.detach_()
+            param.grad.zero_()
+    for param in stash.all_fp32_from_fp32_params:
+        if param.grad is not None:
+            param.grad.detach_()
+            param.grad.zero_()
+    # Clear the master grads that are independent of model grads
+    for param in stash.all_fp32_from_fp16_params:
+        param.grad = None
+
+
+def new_zero_grad_accelerate_with_master_weights(self):
+    stash = self._amp_stash
+    self._amp_lazy_init()
+    self._check_already_combined_params_and_grads()
+    # Zero the model grads.
+    stash.process_zero_grad = True
+
+    if not stash.already_combined:
+        for param in stash.all_fp16_params:
+            if param.grad is not None:
+                param.grad.detach_()
+                param.grad.zero_()
+        for param in stash.all_fp32_from_fp32_params:
+            if param.grad is not None:
+                param.grad.detach_()
+                param.grad.zero_()
+        for param in stash.all_fp32_from_fp16_params:
+            if param.grad is not None:
+                param.grad.zero_()
+        return
+
+    if stash.main_fp16_grad_combine is not None:
+        stash.main_fp16_grad_combine.zero_()
+    if stash.main_fp32_from_fp32_grad_combine is not None:
+        stash.main_fp32_from_fp32_grad_combine.zero_()
+    # Clear the master grads that are independent of model grads
+    if stash.main_fp32_from_fp16_grad_combine is not None:
+        stash.main_fp32_from_fp16_grad_combine.zero_()
+
+
+def can_get_combined_tensors(self, name):
+    if name == 'params':
+        if not self.is_npu_fused_optimizer:
+            maybe_print("To get combined params, please use npu fused optimizer.")
+            return False
+    elif name == 'grads' or name == 'grad_masks':
+        if (not self.accelerate) and (not self.is_npu_fused_optimizer):
+            maybe_print("To get combined {}, please set combine_grad=True or use npu fused optimizer.".format(name))
+            return False
+    else:
+        maybe_print("{} are not supported to be combined.".format(name))
+        return False
+
+    stash = self._amp_stash
+    if not stash.already_combined:
+        maybe_print("Please get the combined {} after backward phase.".format(name))
+        return False
+    return True
+
+
+def get_model_combined_params(self):
+    stash = self._amp_stash
+    combined_params = []
+
+    if not self._can_get_combined_tensors('params'):
+        return combined_params
+
+    self._check_already_combined_params_and_grads()
+    self._amp_combined_init()
+
+    if stash.master_weights:
+        combined_params.append(stash.main_fp16_param_combine)
+        combined_params.append(stash.main_fp32_from_fp32_param_combine)
+    else:
+        combined_params.append(stash.main_fp32_param_combine)
+    return combined_params
+
+
+def get_model_combined_grads(self):
+    stash = self._amp_stash
+    combined_grads = []
+
+    if not self._can_get_combined_tensors('grads'):
+        return combined_grads
+
+    self._check_already_combined_params_and_grads()
+    self._amp_combined_init()
+
+    if stash.master_weights:
+        combined_grads.append(stash.main_fp16_grad_combine)
+        combined_grads.append(stash.main_fp32_from_fp32_grad_combine)
+    else:
+        combined_grads.append(stash.main_fp32_grad_combine)
+    return combined_grads
+
+
+def get_optimizer_combined_params(self):
+    stash = self._amp_stash
+    combined_params = []
+
+    if not self._can_get_combined_tensors('params'):
+        return combined_params
+
+    self._check_already_combined_params_and_grads()
+    self._amp_combined_init()
+
+    if stash.master_weights:
+        combined_params.append(stash.main_fp32_from_fp16_param_combine)
+        combined_params.append(stash.main_fp32_from_fp32_param_combine)
+    else:
+        combined_params.append(stash.main_fp32_param_combine)
+    return combined_params
+
+
+def get_optimizer_combined_grads(self):
+    stash = self._amp_stash
+    combined_grads = []
+
+    if not self._can_get_combined_tensors('grads'):
+        return combined_grads
+
+    self._check_already_combined_params_and_grads()
+    self._amp_combined_init()
+
+    if stash.master_weights:
+        combined_grads.append(stash.main_fp32_from_fp16_grad_combine)
+        combined_grads.append(stash.main_fp32_from_fp32_grad_combine)
+    else:
+        combined_grads.append(stash.main_fp32_grad_combine)
+    return combined_grads
+
+
+def get_optimizer_combined_grad_masks(self):
+    stash = self._amp_stash
+    combined_grad_masks = []
+
+    if not self._can_get_combined_tensors('grad_masks'):
+        return combined_grad_masks
+
+    if stash.master_weights:
+        if stash.main_fp32_from_fp16_grad_combine_mask is None:
+            stash.main_fp32_from_fp16_grad_combine_mask = \
+                get_grad_combined_tensor_mask_from_param(stash.all_fp32_from_fp16_params)
+            stash.main_fp32_from_fp32_grad_combine_mask = \
+                get_grad_combined_tensor_mask_from_param(stash.all_fp32_from_fp32_params)
+        combined_grad_masks.append(stash.main_fp32_from_fp16_grad_combine_mask)
+        combined_grad_masks.append(stash.main_fp32_from_fp32_grad_combine_mask)
+    else:
+        if stash.main_fp32_grad_combine_mask is None:
+            stash.main_fp32_grad_combine_mask = \
+                get_grad_combined_tensor_mask_from_param(stash.all_fp32_params)
+        combined_grad_masks.append(stash.main_fp32_grad_combine_mask)
+    return combined_grad_masks
+
+
 def _process_optimizer(optimizer, properties):
     if hasattr(optimizer, "_amp_stash"):
         raise RuntimeError("A given optimizer should only be passed through amp.initialize once.")
@@ -327,15 +931,64 @@
     optimizer._amp_stash.lazy_init_called = False
     optimizer._amp_stash.already_patched = False
     optimizer._amp_stash.params_have_scaled_gradients = False
+    optimizer.accelerate = properties.combine_grad
+    optimizer.check_combined_tensors = properties.check_combined_tensors
+    optimizer._amp_stash.master_weights = properties.master_weights
+    optimizer._amp_stash.grads_list = []
+    optimizer._amp_stash.already_combined = False
+
+    optimizer._amp_stash.process_zero_grad = True
+
+    optimizer._amp_stash.params_grads_are_combined_by_group = False
+    optimizer._amp_stash.combined_params_indexed_by_group = []
+    optimizer._amp_stash.combined_grads_indexed_by_group = []
+    optimizer._amp_stash.params_lists_indexed_by_group = []
+    optimizer._amp_stash.param_states_are_combined_by_group = False
+    optimizer._amp_stash.combined_param_states_indexed_by_group = []
 
     for name in ("_lazy_init_maybe_master_weights",
                  "_master_params_to_model_params",
                  "_prepare_amp_backward",
                  "_post_amp_backward",
-                 "_amp_lazy_init"):
+                 "_amp_lazy_init",
+                 "_amp_combined_init",
+                 "_reset_all_combine_flags",
+                 "_check_already_combined_params_and_grads",
+                 "_combine_params_and_grads_by_group",
+                 "_can_get_combined_tensors",
+                 "get_model_combined_params",
+                 "get_model_combined_grads",
+                 "get_optimizer_combined_params",
+                 "get_optimizer_combined_grads"):
         if hasattr(optimizer, name):
             raise RuntimeError("Incoming optimizer already has {} defined.".format(name))
 
+    if properties.opt_level == "O2" and properties.combine_grad and properties.master_weights != True:
+        raise RuntimeError("With opt_level O2, master_weights should be True when combine_grad is True")
+
+    if hasattr(optimizer, "is_npu_fused_optimizer") and optimizer.is_npu_fused_optimizer is True:
+        maybe_print("Use npu fused optimizer")
+    else:
+        optimizer.is_npu_fused_optimizer = False
+
+    if optimizer.is_npu_fused_optimizer:
+        if properties.opt_level != "O1" and properties.opt_level != "O2":
+            raise RuntimeError("Currently, npu fused optimizer can only be used when opt_level='O1' or opt_level='O2'")
+
+        if properties.opt_level == "O2" and properties.master_weights != True:
+            raise RuntimeError("With opt_level O2, master_weights should be True when npu fused optimizer is used")
+
+        old_load_state_dict = optimizer.load_state_dict
+        def new_load_state_dict(self, state_dict):
+            old_load_state_dict(state_dict)
+            self._amp_stash.param_states_are_combined_by_group = False
+        optimizer.load_state_dict = types.MethodType(new_load_state_dict, optimizer)
+
+    if not properties.combine_grad and not optimizer.is_npu_fused_optimizer and \
+        properties.check_combined_tensors:
+        maybe_print("Because combine_grad != True and no npu fused optimizer is used, "
+                    "checking combined tensors function will not take effect!")
+
     # TODO:  Centralize exposure and import error checking for the C backend.
     if multi_tensor_applier.available:
         import amp_C
@@ -352,34 +1005,31 @@
 
         old_step = optimizer.step
         def new_step(self, closure=None):
+            stash = self._amp_stash
             if closure is not None:
                 raise RuntimeError("Currently, Amp does not support closure use with optimizers.")
             retval = old_step()
             if not isinstance(self, FusedSGD):
                 self._master_params_to_model_params()
             # Clear the master grads that wouldn't be zeroed by model.zero_grad()
-            for param in self._amp_stash.all_fp32_from_fp16_params:
-                param.grad = None
+            if optimizer.accelerate or optimizer.is_npu_fused_optimizer:
+                if stash.main_fp32_from_fp16_grad_combine is not None:
+                    stash.main_fp32_from_fp16_grad_combine.zero_()
+            else:
+                for param in stash.all_fp32_from_fp16_params:
+                    param.grad = None
             return retval
         optimizer.step = types.MethodType(new_step, optimizer)
 
         old_zero_grad = optimizer.zero_grad
-        def new_zero_grad(self):
-            stash = self._amp_stash
-            self._amp_lazy_init()
-            # Zero the model grads.
-            for param in stash.all_fp16_params:
-                if param.grad is not None:
-                    param.grad.detach_()
-                    param.grad.zero_()
-            for param in stash.all_fp32_from_fp32_params:
-                if param.grad is not None:
-                    param.grad.detach_()
-                    param.grad.zero_()
-            # Clear the master grads that are independent of model grads
-            for param in self._amp_stash.all_fp32_from_fp16_params:
-                param.grad = None
-        optimizer.zero_grad = types.MethodType(new_zero_grad, optimizer)
+        if optimizer.accelerate or optimizer.is_npu_fused_optimizer:
+            optimizer.zero_grad = types.MethodType(new_zero_grad_accelerate_with_master_weights, optimizer)
+        else:
+            optimizer.zero_grad = types.MethodType(new_zero_grad_with_master_weights, optimizer)
+
+        if optimizer.is_npu_fused_optimizer:
+            optimizer._combine_params_and_grads_by_group = types.MethodType(
+                combine_params_and_grads_by_group_with_master_weights, optimizer)
 
         if isinstance(optimizer, FusedSGD):
             optimizer._prepare_amp_backward = types.MethodType(
@@ -391,10 +1041,35 @@
                 prepare_backward_with_master_weights, optimizer)
             optimizer._post_amp_backward = types.MethodType(
                 post_backward_with_master_weights, optimizer)
+        
+        optimizer._amp_combined_init = types.MethodType(combined_init_with_master_weights, optimizer)
+        optimizer._check_already_combined_params_and_grads = types.MethodType(
+            check_already_combined_params_and_grads_with_master_weights, optimizer)
     else:
         optimizer._lazy_init_maybe_master_weights = types.MethodType(
             lazy_init_no_master_weights, optimizer)
 
+        old_zero_grad = optimizer.zero_grad
+        if optimizer.accelerate or optimizer.is_npu_fused_optimizer:
+            def new_zero_grad_accelerate_no_master_weights(self):
+                stash = self._amp_stash
+                self._amp_lazy_init()
+                self._check_already_combined_params_and_grads()
+                # Zero the model grads.
+                if not stash.already_combined:
+                    old_zero_grad()
+                    return
+
+                if stash.main_fp16_grad_combine is not None:
+                    stash.main_fp16_grad_combine.zero_()
+                if stash.main_fp32_grad_combine is not None:
+                    stash.main_fp32_grad_combine.zero_()
+            optimizer.zero_grad = types.MethodType(new_zero_grad_accelerate_no_master_weights, optimizer)
+
+        if optimizer.is_npu_fused_optimizer:
+            optimizer._combine_params_and_grads_by_group = types.MethodType(
+                combine_params_and_grads_by_group_no_master_weights, optimizer)
+
         if isinstance(optimizer, FusedSGD):
             optimizer._prepare_amp_backward = types.MethodType(
                 prepare_backward_no_master_weights_FusedSGD, optimizer)
@@ -406,7 +1081,18 @@
             optimizer._post_amp_backward = types.MethodType(
                 post_backward_no_master_weights, optimizer)
 
+        optimizer._amp_combined_init = types.MethodType(combined_init_no_master_weights, optimizer)
+        optimizer._check_already_combined_params_and_grads = types.MethodType(
+            check_already_combined_params_and_grads_no_master_weights, optimizer)
+
     optimizer._amp_lazy_init = types.MethodType(_amp_lazy_init, optimizer)
+    optimizer._reset_all_combine_flags = types.MethodType(reset_all_combine_flags, optimizer)
+    optimizer._can_get_combined_tensors = types.MethodType(can_get_combined_tensors, optimizer)
+    optimizer.get_model_combined_params = types.MethodType(get_model_combined_params, optimizer)
+    optimizer.get_model_combined_grads = types.MethodType(get_model_combined_grads, optimizer)
+    optimizer.get_optimizer_combined_params = types.MethodType(get_optimizer_combined_params, optimizer)
+    optimizer.get_optimizer_combined_grads = types.MethodType(get_optimizer_combined_grads, optimizer)
+    optimizer.get_optimizer_combined_grad_masks = types.MethodType(get_optimizer_combined_grad_masks, optimizer)
 
     old_add_param_group = optimizer.add_param_group
 
@@ -435,13 +1121,13 @@
             fp32_from_fp16_params_this_group = []
             for i, param in enumerate(new_group['params']):
                 if param.requires_grad:
-                    if param.type() == 'torch.cuda.HalfTensor':
+                    if param.type() == 'torch.npu.HalfTensor':
                         fp16_params_this_group.append(param)
                         master_param = param.detach().clone().float()
                         master_param.requires_grad = True
                         new_group['params'][i] = master_param
                         fp32_from_fp16_params_this_group.append(master_param)
-                    elif param.type() == 'torch.cuda.FloatTensor':
+                    elif param.type() == 'torch.npu.FloatTensor':
                         fp32_params_this_group.append(param)
                         new_group['params'][i] = param
                     else:
@@ -471,10 +1157,10 @@
             #     param.grad = None
         else:
             for param in new_group['params']:
-                if param.type() == 'torch.cuda.HalfTensor':
+                if param.type() == 'torch.npu.HalfTensor':
                     stash.all_fp16_params.append(param)
                     stash.all_fp16_grad_stash.append(None)
-                elif param.type() == 'torch.cuda.FloatTensor':
+                elif param.type() == 'torch.npu.FloatTensor':
                     stash.all_fp32_params.append(param)
                     stash.all_fp32_grad_stash.append(None)
                 else:
diff -Nur '--exclude=.git' apex/apex/amp/frontend.py apex-npu/apex/amp/frontend.py
--- apex/apex/amp/frontend.py	2021-04-12 04:03:22.000000000 +0000
+++ apex-npu/apex/amp/frontend.py	2021-06-17 07:10:45.373711948 +0000
@@ -19,6 +19,8 @@
             "keep_batchnorm_fp32" : None,
             "master_weights" : None,
             "loss_scale" : 1.0,
+            "combine_grad": None,
+            "check_combined_tensors": None
             # Reserved for future functionality
             # "fused_optimizer" : False,
             # "enable_ddp_interop" : False,
@@ -91,6 +93,11 @@
                         self.options[name] = value
                     else:
                         self.options[name] = float(value)
+                elif name == "combine_grad" or name == "check_combined_tensors":
+                    if self.opt_level not in ["O1", "O2"] and value:
+                        warn_or_err("Currently, combine_grad=True or check_combined_tensors=True should only be set "
+                                    "by selecting opt_level='O1' or opt_level='O2'.")
+                    self.options[name] = value
                 else:
                     self.options[name] = value
         else:
@@ -161,6 +168,7 @@
         properties.keep_batchnorm_fp32 = None
         properties.master_weights = None
         properties.loss_scale = "dynamic"
+        properties.combine_grad = None
         # properties.fused_optimizer = False
         # properties.enable_ddp_interop = False
         return properties # modified in place so this isn't really necessary
@@ -206,7 +214,9 @@
     num_losses=1,
     verbosity=1,
     min_loss_scale=None,
-    max_loss_scale=2.**24
+    max_loss_scale=2.**24,
+    combine_grad=None,
+    check_combined_tensors=None
     ):
     """
     Initialize your models, optimizers, and the Torch tensor and functional namespace according to the
@@ -259,6 +269,9 @@
             If dynamic loss scaling is not used, `min_loss_scale` is ignored.
         max_loss_scale (float, default=2.**24):  Sets a ceiling for the loss scale values that can be chosen by
             dynamic loss scaling.  If dynamic loss scaling is not used, `max_loss_scale` is ignored.
+        combine_grad (bool, optional, default=None): If True, make gradients fused for unscale.
+        check_combined_tensors (bool, optional, default=None): If True, check if the combined grads and combined params
+            are valid during training
 
     Returns:
         Model(s) and optimizer(s) modified according to the ``opt_level``.
@@ -306,6 +319,7 @@
         https://github.com/NVIDIA/apex/issues
     """
     _amp_state.opt_properties = Properties()
+    # Here add a switch to open combine tensor
     _amp_state.verbosity = verbosity
 
     if not enabled:
@@ -350,6 +364,10 @@
         _amp_state.opt_properties.master_weights = master_weights
     if loss_scale is not None:
         _amp_state.opt_properties.loss_scale = loss_scale
+    if combine_grad is not None:
+        _amp_state.opt_properties.combine_grad = combine_grad
+    if check_combined_tensors is not None:
+        _amp_state.opt_properties.check_combined_tensors = check_combined_tensors
 
     maybe_print("After processing overrides, optimization options are:", True)
     for k, v in _amp_state.opt_properties.options.items():
diff -Nur '--exclude=.git' apex/apex/amp/handle.py apex-npu/apex/amp/handle.py
--- apex/apex/amp/handle.py	2021-04-12 04:03:22.000000000 +0000
+++ apex-npu/apex/amp/handle.py	2021-06-17 07:10:45.373711948 +0000
@@ -1,3 +1,19 @@
+# Copyright (c) 2020, Huawei Technologies.
+# Copyright (c) 2019, NVIDIA CORPORATION.
+# All rights reserved.
+#
+# Licensed under the BSD 3-Clause License  (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+# https://opensource.org/licenses/BSD-3-Clause
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
 import contextlib
 import warnings
 import sys
@@ -110,6 +126,9 @@
                 if not optimizer._amp_stash.params_have_scaled_gradients:
                     optimizer._prepare_amp_backward()
 
+    if loss_scaler.dynamic:
+        LossScaler.clear_npu_overflow_flag()
+
     yield (loss.float())*loss_scale
 
     if delay_unscale:
@@ -142,8 +161,12 @@
                                 # Maybe skip should delegate to a method owned by the optimizers themselves.
                                 if hasattr(opt._amp_stash, "all_fp32_from_fp16_params"):
                                     # Clear the master grads that wouldn't be zeroed by model.zero_grad()
-                                    for param in opt._amp_stash.all_fp32_from_fp16_params:
-                                        param.grad = None
+                                    if opt.accelerate or opt.is_npu_fused_optimizer:
+                                        if opt._amp_stash.main_fp32_from_fp16_grad_combine is not None:
+                                            opt._amp_stash.main_fp32_from_fp16_grad_combine.zero_()
+                                    else:
+                                        for param in opt._amp_stash.all_fp32_from_fp16_params:
+                                            param.grad = None
                                 if hasattr(opt, "most_recent_scale"):
                                     opt.most_recent_scale = 1.0
                                     opt.scale_set_by_backward = False
diff -Nur '--exclude=.git' apex/apex/amp/scaler.py apex-npu/apex/amp/scaler.py
--- apex/apex/amp/scaler.py	2021-04-12 04:03:22.000000000 +0000
+++ apex-npu/apex/amp/scaler.py	2021-06-17 07:10:45.373711948 +0000
@@ -1,7 +1,25 @@
+# Copyright (c) 2020, Huawei Technologies.
+# Copyright (c) 2019, NVIDIA CORPORATION.
+# All rights reserved.
+#
+# Licensed under the BSD 3-Clause License  (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+# https://opensource.org/licenses/BSD-3-Clause
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
 import torch
+import torch.distributed as dist
 from ..multi_tensor_apply import multi_tensor_applier
 from ._amp_state import _amp_state, master_params, maybe_print
 from itertools import product
+import importlib
 
 def scale_check_overflow_python(model_grad, master_grad, scale, check_overflow=False):
     # Exception handling for 18.04 compatibility
@@ -16,7 +34,8 @@
         master_grad.mul_(scale)
     return False
 
-def axpby_check_overflow_python(model_grad, stashed_grad, master_grad, a, b, check_overflow=False):
+def axpby_check_overflow_python(model_grad, stashed_grad, master_grad, a, b, use_npu_fused_optimizer, 
+                                check_overflow=False):
     # Exception handling for 18.04 compatibility
     if check_overflow:
         cpu_sum = float(model_grad.float().sum())
@@ -27,13 +46,17 @@
     #     master_grad.copy_(model_grad)
     assert stashed_grad.dtype == master_grad.dtype
     converted_model_grad = model_grad.data.to(master_grad.dtype)
-    master_grad.data = a*converted_model_grad.data + b*stashed_grad.data
+    if use_npu_fused_optimizer:
+        master_grad.data[:] = a*converted_model_grad.data + b*stashed_grad.data
+    else:
+        master_grad.data = a*converted_model_grad.data + b*stashed_grad.data
     return False
 
 class LossScaler(object):
     warned_no_fused_kernel = False
     warned_unscaling_non_fp32_grad = False
     has_fused_kernel = False
+    npu_float_status = None
 
     def __init__(self,
                  loss_scale,
@@ -45,50 +68,89 @@
         if loss_scale == "dynamic":
             self.dynamic = True
             self._loss_scale = min(max_loss_scale, init_scale)
+            try:
+                LossScaler.npu_float_status = importlib.import_module("npu_float_status")
+            except ModuleNotFoundError as module_err:
+                maybe_print(
+                    '\nImport module "npu_float_status" failed, '
+                    'please install apex with --global-option="--npu_float_status" and then try again!\n')
+                raise(module_err)
+            except Exception as other_err:
+                raise(other_err)
         else:
             self.dynamic = False
             self._loss_scale = loss_scale
+            LossScaler.npu_float_status = None
         self._max_loss_scale = max_loss_scale
         self._min_loss_scale = min_loss_scale
         self._scale_seq_len = scale_window
         self._unskipped = 0
         self._has_overflow = False
-        self._overflow_buf = torch.cuda.IntTensor([0])
+        self._overflow_buf = torch.npu.IntTensor([0])
+        self._dist_overflow_count = torch.Tensor([0.]).to('npu')
+        self._dist_initialized = False
+
+        try:
+            if dist.is_initialized():
+                self._dist_initialized = True
+        except AttributeError as err:
+            maybe_print("torch.distributed has no attribute is_initialized")
+
         if multi_tensor_applier.available:
             import amp_C
             LossScaler.has_fused_kernel = multi_tensor_applier.available
             LossScaler.multi_tensor_scale_cuda = amp_C.multi_tensor_scale
             LossScaler.multi_tensor_axpby_cuda = amp_C.multi_tensor_axpby
         else:
-            if not LossScaler.warned_no_fused_kernel:
-                maybe_print(
-                    "Warning:  multi_tensor_applier fused unscale kernel is unavailable, "
-                    "possibly because apex was installed without --cuda_ext --cpp_ext. "
-                    "Using Python fallback.  Original ImportError was: " +
-                    repr(multi_tensor_applier.import_err),
-                    True)
             LossScaler.has_fused_kernel = False
             LossScaler.warned_no_fused_kernel = True
 
+    @staticmethod
+    def get_npu_overflow_flag():
+        if LossScaler.npu_float_status is not None:
+            return LossScaler.npu_float_status.RunGetFloatStatusOp()
+        else:
+            return False
+    
+    @staticmethod
+    def clear_npu_overflow_flag():
+        if LossScaler.npu_float_status is not None:
+            LossScaler.npu_float_status.RunClearFloatStatusOp()
+
     def loss_scale(self):
         return self._loss_scale
 
     def unscale_python(self, model_grads, master_grads, scale):
-        for model, master in zip(model_grads, master_grads):
-            if model is not None:
-                if not LossScaler.warned_unscaling_non_fp32_grad:
-                    if master.dtype != torch.float32:
-                        maybe_print(
-                            "Attempting to unscale a grad with type {} ".format(master.type()) +
-                            "Unscaling non-fp32 grads may indicate an error. "
-                            "When using Amp, you don't need to call .half() on your model.")
-                        LossScaler.warned_unscaling_non_fp32_grad = True
-                self._has_overflow = scale_check_overflow_python(model,
-                                                                 master,
-                                                                 1./scale,
-                                                                 self.dynamic)
-                if self._has_overflow and self.dynamic:
-                    break
+        if self.dynamic:
+            self._has_overflow = LossScaler.get_npu_overflow_flag()
+        else:
+            self._has_overflow = False
+
+        if not self._has_overflow:
+            for model, master in zip(model_grads, master_grads):
+                if model is not None:
+                    if not LossScaler.warned_unscaling_non_fp32_grad:
+                        if master.dtype != torch.float32:
+                            maybe_print(
+                                "Attempting to unscale a grad with type {} ".format(master.type()) +
+                                "Unscaling non-fp32 grads may indicate an error. "
+                                "When using Amp, you don't need to call .half() on your model.")
+                            LossScaler.warned_unscaling_non_fp32_grad = True
+                    self._has_overflow = scale_check_overflow_python(model,
+                                                                     master,
+                                                                     1./scale)
+
+        if self._has_overflow:
+            if self.dynamic and self._dist_initialized:
+                self._dist_overflow_count.add_(1)
+                dist.all_reduce(self._dist_overflow_count)
+                self._dist_overflow_count.zero_()
+        else:
+            if self.dynamic and self._dist_initialized:
+                dist.all_reduce(self._dist_overflow_count)
+                if self._dist_overflow_count.item() != 0:
+                    self._has_overflow = True
+                self._dist_overflow_count.zero_()
 
     # unused_scale keeps some of the old API alive for hopefully a short time.
     def unscale(self, model_grads, master_grads, unused_scale, models_are_masters=False, scale_override=None):
@@ -117,7 +179,7 @@
                                  1./scale)
         else:
             self.unscale_python(model_grads, master_grads, scale)
-
+        
         # Defer to update_scale
         # If the fused kernel is available, we only need one D2H memcopy and sync.
         # if LossScaler.has_fused_kernel and self.dynamic and not self._has_overflow:
@@ -128,32 +190,50 @@
                                     stashed_master_grads,
                                     master_grads,
                                     a,
-                                    b):
-        for model, stashed, master in zip(model_grads, stashed_master_grads, master_grads):
-            if model is None and stashed is None:
-                continue
-            else:
-                if not LossScaler.warned_unscaling_non_fp32_grad:
-                    if master.dtype != torch.float32:
-                        maybe_print(
-                            "Attempting to unscale a grad with type {} ".format(master.type()) +
-                            "Unscaling non-fp32 grads may indicate an error. "
-                            "When using Amp, you don't need to call .half() on your model.")
-                        LossScaler.warned_unscaling_non_fp32_grad = True
-                self._has_overflow = axpby_check_overflow_python(model,
-                                                                 stashed,
-                                                                 master,
-                                                                 a,
-                                                                 b,
-                                                                 self.dynamic)
-                if self._has_overflow and self.dynamic:
-                    break
+                                    b,
+                                    use_npu_fused_optimizer):
+        if self.dynamic:
+            self._has_overflow = LossScaler.get_npu_overflow_flag()
+        else:
+            self._has_overflow = False
+
+        if not self._has_overflow:
+            for model, stashed, master in zip(model_grads, stashed_master_grads, master_grads):
+                if model is None and stashed is None:
+                    continue
+                else:
+                    if not LossScaler.warned_unscaling_non_fp32_grad:
+                        if master.dtype != torch.float32:
+                            maybe_print(
+                                "Attempting to unscale a grad with type {} ".format(master.type()) +
+                                "Unscaling non-fp32 grads may indicate an error. "
+                                "When using Amp, you don't need to call .half() on your model.")
+                            LossScaler.warned_unscaling_non_fp32_grad = True
+                    self._has_overflow = axpby_check_overflow_python(model,
+                                                                     stashed,
+                                                                     master,
+                                                                     a,
+                                                                     b,
+                                                                     use_npu_fused_optimizer)
+
+        if self._has_overflow:
+            if self.dynamic and self._dist_initialized:
+                self._dist_overflow_count.add_(1)
+                dist.all_reduce(self._dist_overflow_count)
+                self._dist_overflow_count.zero_()
+        else:
+            if self.dynamic and self._dist_initialized:
+                dist.all_reduce(self._dist_overflow_count)
+                if self._dist_overflow_count.item() != 0:
+                    self._has_overflow = True
+                self._dist_overflow_count.zero_()
 
     def unscale_with_stashed(self,
                              model_grads,
                              stashed_master_grads,
                              master_grads,
-                             scale_override=None):
+                             scale_override=None,
+                             use_npu_fused_optimizer=False):
         if self._has_overflow:
             return
 
@@ -181,13 +261,86 @@
                                              stashed_master_grads,
                                              master_grads,
                                              out_scale/grads_have_scale,
-                                             out_scale/stashed_have_scale)
+                                             out_scale/stashed_have_scale,
+                                             use_npu_fused_optimizer)
 
         # Defer to update_scale
         # If the fused kernel is available, we only need one D2H memcopy and sync.
         # if LossScaler.has_fused_kernel and self.dynamic and not self._has_overflow:
         #     self._has_overflow = self._overflow_buf.item()
 
+    def unscale_with_stashed_combined(self,
+                                      grads_combined,
+                                      stashed_grads_combined,
+                                      scale_override=None):
+
+        if self.dynamic:
+            self._has_overflow = LossScaler.get_npu_overflow_flag()
+            if self._dist_initialized:
+                if self._has_overflow:
+                    self._dist_overflow_count.add_(1)
+                    dist.all_reduce(self._dist_overflow_count)
+                    self._dist_overflow_count.zero_()
+                else:
+                    dist.all_reduce(self._dist_overflow_count)
+                    if self._dist_overflow_count.item() != 0:
+                        self._has_overflow = True
+                    self._dist_overflow_count.zero_()
+
+        if self._has_overflow:
+            return
+        
+        grads_have_scale, stashed_have_scale, out_scale = self._loss_scale, 1.0, 1.0
+        if scale_override is not None:
+            grads_have_scale, stashed_have_scale, out_scale = scale_override
+
+        grads_combined.data[:] = grads_combined.mul_(out_scale/grads_have_scale) + stashed_grads_combined
+
+    def unscale_grad_O2(self,
+                        model_grads_combined=None,
+                        stashed_master_grads_combined=None,
+                        master_grads_combined=None,
+                        scale_override=None,
+                        master_grads=None,
+                        model_grads=None):
+
+        if self.dynamic:
+            self._has_overflow = LossScaler.get_npu_overflow_flag()
+            if self._dist_initialized:
+                if self._has_overflow:
+                    self._dist_overflow_count.add_(1)
+                    dist.all_reduce(self._dist_overflow_count)
+                    self._dist_overflow_count.zero_()
+                else:
+                    dist.all_reduce(self._dist_overflow_count)
+                    if self._dist_overflow_count.item() != 0:
+                        self._has_overflow = True
+                    self._dist_overflow_count.zero_()
+
+        if self._has_overflow:
+            return
+        
+        grads_have_scale, stashed_have_scale, out_scale = self._loss_scale, 1.0, 1.0
+        if scale_override is not None:
+            grads_have_scale, stashed_have_scale, out_scale = scale_override
+
+        if stashed_master_grads_combined is not None and \
+                master_grads_combined.data_ptr() == stashed_master_grads_combined.data_ptr() and \
+                master_grads_combined.numel() == stashed_master_grads_combined.numel():
+            stashed_master_grads_combined = master_grads_combined.clone()
+
+        if master_grads_combined is not model_grads_combined:
+            if master_grads_combined.numel() == model_grads_combined.numel():
+                master_grads_combined.copy_(model_grads_combined)
+            else:
+                for master, model in zip(master_grads, model_grads):
+                    master.copy_(model)
+        master_grads_combined.mul_(out_scale/grads_have_scale)
+
+        if stashed_master_grads_combined is not None:
+            assert stashed_master_grads_combined.dtype == master_grads_combined.dtype
+            master_grads_combined.add_(stashed_master_grads_combined)
+
     def clear_overflow_state(self):
         self._has_overflow = False
         if self.has_fused_kernel:
diff -Nur '--exclude=.git' apex/apex/amp/utils.py apex-npu/apex/amp/utils.py
--- apex/apex/amp/utils.py	2021-04-12 04:03:22.000000000 +0000
+++ apex-npu/apex/amp/utils.py	2021-06-17 07:10:45.377711979 +0000
@@ -1,3 +1,19 @@
+# Copyright (c) 2020, Huawei Technologies.
+# Copyright (c) 2019, NVIDIA CORPORATION.
+# All rights reserved.
+#
+# Licensed under the BSD 3-Clause License  (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+# https://opensource.org/licenses/BSD-3-Clause
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
 from . import compat
 
 import functools
@@ -55,7 +71,7 @@
     if is_nested(x):
         return type(x)([maybe_half(y) for y in x])
 
-    if not x.is_cuda or type_string(x) == 'HalfTensor':
+    if not 'npu' in x.type()  or type_string(x) == 'HalfTensor':
         return x
     else:
         if verbose:
@@ -66,7 +82,7 @@
     if is_nested(x):
         return type(x)([maybe_float(y) for y in x])
 
-    if not x.is_cuda or type_string(x) == 'FloatTensor':
+    if not 'npu' in x.type() or type_string(x) == 'FloatTensor':
         return x
     else:
         if verbose:
@@ -94,7 +110,7 @@
         cached_x = cache[x]
         if x.requires_grad and cached_x.requires_grad:
             # Make sure x is actually cached_x's autograd parent.
-            if cached_x.grad_fn.next_functions[1][0].variable is not x:
+            if cached_x.grad_fn.next_functions[0][0].variable is not x:
                 raise RuntimeError("x and cache[x] both require grad, but x is not "
                                    "cache[x]'s parent.  This is likely an error.")
         # During eval, it's possible to end up caching casted weights with
diff -Nur '--exclude=.git' apex/apex/contrib/sparsity/README.md apex-npu/apex/contrib/sparsity/README.md
--- apex/apex/contrib/sparsity/README.md	2021-04-12 04:03:22.000000000 +0000
+++ apex-npu/apex/contrib/sparsity/README.md	2021-06-17 07:10:45.389712070 +0000
@@ -1,37 +1,37 @@
-# Introduction to ASP
-
-This page documents the API for ASP (Automatic Sparsity), a tool that enables sparse training and inference for PyTorch models by adding 2 lines of Python.
-
-## Importing ASP
-```
-from apex.contrib.sparsity import ASP
-```
-
-## Initializing ASP
-
-Apart from the import statement, it is sufficient to add just the following line of code before the training phase to augment the model and the optimizer for sparse training/infercence:
-```
-ASP.prune_trained_model(model, optimizer)
-```
-
-In a typical PyTorch training loop, it might look like this:
-
-```
-ASP.prune_trained_model(model, optimizer)
-
-x, y = DataLoader(args)
-for epoch in range(epochs):
-    y_pred = model(x)
-    loss = loss_function(y_pred, y)
-    loss.backward()
-    optimizer.step()
-
-torch.save(...)
-```
-The `prune_trained_model` calculates the sparse mask and applies it to the weights. This is done once, i.e., sparse locations in the weights matrix remain fixed after this step. In order to recompute the sparse mask in between training, say after an epoch, use the following method:
-
-```
-ASP.compute_sparse_masks()
-```
-
+# Introduction to ASP
+
+This page documents the API for ASP (Automatic Sparsity), a tool that enables sparse training and inference for PyTorch models by adding 2 lines of Python.
+
+## Importing ASP
+```
+from apex.contrib.sparsity import ASP
+```
+
+## Initializing ASP
+
+Apart from the import statement, it is sufficient to add just the following line of code before the training phase to augment the model and the optimizer for sparse training/infercence:
+```
+ASP.prune_trained_model(model, optimizer)
+```
+
+In a typical PyTorch training loop, it might look like this:
+
+```
+ASP.prune_trained_model(model, optimizer)
+
+x, y = DataLoader(args)
+for epoch in range(epochs):
+    y_pred = model(x)
+    loss = loss_function(y_pred, y)
+    loss.backward()
+    optimizer.step()
+
+torch.save(...)
+```
+The `prune_trained_model` calculates the sparse mask and applies it to the weights. This is done once, i.e., sparse locations in the weights matrix remain fixed after this step. In order to recompute the sparse mask in between training, say after an epoch, use the following method:
+
+```
+ASP.compute_sparse_masks()
+```
+
 A more thorough example can be found in `./test/toy_problem.py`. 
\ No newline at end of file
diff -Nur '--exclude=.git' apex/apex/fp16_utils/fp16_optimizer.py apex-npu/apex/fp16_utils/fp16_optimizer.py
--- apex/apex/fp16_utils/fp16_optimizer.py	2021-04-12 04:03:22.000000000 +0000
+++ apex-npu/apex/fp16_utils/fp16_optimizer.py	2021-06-17 07:10:45.389712070 +0000
@@ -1,554 +1,554 @@
-import torch
-from torch import nn
-from torch.autograd import Variable
-from torch.nn.parameter import Parameter
-from torch._utils import _flatten_dense_tensors, _unflatten_dense_tensors
-
-from ..amp._amp_state import _amp_state, maybe_print
-from ..amp.scaler import LossScaler
-from ..multi_tensor_apply import multi_tensor_applier
-from .fp16util import model_grads_to_master_grads, master_params_to_model_params, clip_grad_norm
-
-# TODO:  Update overflow check + downscale to use Carl's fused kernel.
-class FP16_Optimizer(object):
-    def __init__(self, 
-                 init_optimizer, 
-                 static_loss_scale=1.0, 
-                 dynamic_loss_scale=False,
-                 dynamic_loss_args=None,
-                 verbose=True):
-        print("Warning:  FP16_Optimizer is deprecated and dangerous, and will be deleted soon.  "
-              "If it still works, you're probably getting lucky.  "
-              "For mixed precision, use the documented API https://nvidia.github.io/apex/amp.html, with opt_level=O1.")
-
-        if not torch.cuda.is_available:
-            raise SystemError("Cannot use fp16 without CUDA.")
-
-        self.verbose = verbose
-
-        self.optimizer = init_optimizer
-        # init_state_dict sets up an alternative way to cast per-param state tensors.
-        # Stashing here in case https://github.com/pytorch/pytorch/issues/7733 makes it necessary.
-        # init_state_dict = init_optimizer.state_dict()
-
-        self.fp16_groups = []
-        self.fp32_from_fp16_groups = []
-        self.fp32_from_fp32_groups = []
-        for i, param_group in enumerate(self.optimizer.param_groups):
-            self.maybe_print("FP16_Optimizer processing param group {}:".format(i))
-            fp16_params_this_group = []
-            fp32_params_this_group = []
-            fp32_from_fp16_params_this_group = []
-            for i, param in enumerate(param_group['params']):
-                if param.requires_grad:
-                    if param.type() == 'torch.cuda.HalfTensor':
-                        self.maybe_print("FP16_Optimizer received torch.cuda.HalfTensor with {}"
-                                         .format(param.size()))
-                        fp16_params_this_group.append(param)
-                        master_param = param.detach().clone().float()
-                        master_param.requires_grad = True
-                        param_group['params'][i] = master_param
-                        fp32_from_fp16_params_this_group.append(master_param)
-                        # Reset existing state dict key to the new master param.
-                        # We still need to recast per-param state tensors, if any, to FP32.
-                        if param in self.optimizer.state:
-                           self.optimizer.state[master_param] = self.optimizer.state.pop(param) 
-                    elif param.type() == 'torch.cuda.FloatTensor':
-                        self.maybe_print("FP16_Optimizer received torch.cuda.FloatTensor with {}"
-                                         .format(param.size()))
-                        fp32_params_this_group.append(param)
-                        param_group['params'][i] = param
-                    else:
-                        raise TypeError("Wrapped parameters must be either "
-                                        "torch.cuda.FloatTensor or torch.cuda.HalfTensor. "  
-                                        "Received {}".format(param.type()))
-            
-            self.fp16_groups.append(fp16_params_this_group)
-            self.fp32_from_fp16_groups.append(fp32_from_fp16_params_this_group)
-            self.fp32_from_fp32_groups.append(fp32_params_this_group)
-
-        self.all_fp16_params = []
-        for group in self.fp16_groups:
-            self.all_fp16_params += group
-
-        self.all_fp32_from_fp16_params = []
-        for group in self.fp32_from_fp16_groups:
-            self.all_fp32_from_fp16_params += group
-
-        self.all_fp32_from_fp32_params = []
-        for group in self.fp32_from_fp32_groups:
-            self.all_fp32_from_fp32_params += group
-
-        # Leverage state_dict() and load_state_dict() to recast preexisting per-param state tensors
-        self.optimizer.load_state_dict(self.optimizer.state_dict())
-        # alternative way to cast per-param state tensors:
-        # self.optimizer.load_state_dict(init_state_dict)
-
-        if dynamic_loss_scale:
-            self.dynamic_loss_scale = True
-            if dynamic_loss_args is not None:
-                self.loss_scaler = LossScaler("dynamic", **dynamic_loss_args)
-            else:
-                self.loss_scaler = LossScaler("dynamic")
-        else:
-            self.dynamic_loss_scale = False
-            self.loss_scaler = LossScaler(static_loss_scale)
-
-        self.overflow = False
-        self.first_closure_call_this_step = True
-
-        self.clip_grad_norm = clip_grad_norm
-
-        # TODO:  Centralize exposure and import error checking for the C backend.
-        if multi_tensor_applier.available:
-            import amp_C
-            self.multi_tensor_scale = amp_C.multi_tensor_scale
-            self._dummy_overflow_buf = torch.cuda.IntTensor([0]);
-
-    # Having self.maybe_print distinct from _amp_state.maybe_print is another artifact
-    # of having to support FP16_Optimizer separately, for the time being.
-    def maybe_print(self, msg):
-        if self.verbose:
-            print(msg)
-            
-    def __getstate__(self):
-        raise RuntimeError("FP16_Optimizer should be serialized using state_dict().")
-
-    def __setstate__(self, state):
-        raise RuntimeError("FP16_Optimizer should be deserialized using load_state_dict().")
-
-    def zero_grad(self, set_grads_to_None=False):
-        """
-        Zero fp32 and fp16 parameter grads.
-        """
-        # In principle, only the .grad attributes of the model params need to be zeroed,
-        # because gradients are copied into the FP32 master params.  However, we zero
-        # all gradients owned by the optimizer, just to be safe:
-        for group in self.optimizer.param_groups:
-             for p in group['params']:
-                 if set_grads_to_None:
-                     p.grad = None
-                 else:
-                     if p.grad is not None:
-                         p.grad.detach_()
-                         p.grad.zero_()
-
-        # Zero fp16 gradients owned by the model:
-        for fp16_group in self.fp16_groups:
-            for param in fp16_group:
-                if set_grads_to_None:
-                    param.grad = None
-                else:
-                    if param.grad is not None:
-                        param.grad.detach_() # as in torch.optim.optimizer.zero_grad()
-                        param.grad.zero_()
-
-    # Should not be used anymore.
-    # def _check_overflow(self):
-    #     params = []
-    #     for group in self.fp16_groups:
-    #         for param in group:
-    #             params.append(param)
-    #     for group in self.fp32_from_fp32_groups:
-    #         for param in group:
-    #             params.append(param)
-    #     self.overflow = self.loss_scaler.has_overflow(params)
-
-    # def _update_scale(self, has_overflow=False):
-    #     self.loss_scaler.update_scale(has_overflow)
-
-    def _master_params_to_model_params(self):
-        if multi_tensor_applier.available:
-            if len(self.all_fp16_params) > 0:
-                multi_tensor_applier(
-                    self.multi_tensor_scale,
-                    self._dummy_overflow_buf,
-                    [self.all_fp32_from_fp16_params, self.all_fp16_params],
-                    1.0)
-        else:
-            for fp16_group, fp32_from_fp16_group in zip(self.fp16_groups, self.fp32_from_fp16_groups):
-                master_params_to_model_params(fp16_group, fp32_from_fp16_group)
-
-    # To consider:  Integrate distributed with this wrapper by registering a hook on each variable
-    # that does the overflow check, gradient copy + downscale, and fp32 allreduce in a different stream.
-    # def _model_grads_to_master_grads(self):
-    #     for fp16_group, fp32_from_fp16_group in zip(self.fp16_groups, self.fp32_from_fp16_groups):
-    #         model_grads_to_master_grads(fp16_group, fp32_from_fp16_group)
-
-    # def _downscale_master(self):
-    #     if self.loss_scale != 1.0:
-    #         for group in self.optimizer.param_groups:
-    #             for param in group['params']:
-    #                 if param.grad is not None:
-    #                     param.grad.data.mul_(1./self.loss_scale)
-
-    def clip_master_grads(self, max_norm, norm_type=2):
-        """
-        Clips fp32 master gradients via ``torch.nn.utils.clip_grad_norm``.
-
-        Args:
-            max_norm (float or int): max norm of the gradients
-            norm_type (float or int): type of the used p-norm. Can be ``'inf'`` for
-                infinity norm.
-
-        Returns:
-            Total norm of the current fp32 gradients (viewed as a single vector).
-
-        .. warning::
-            Returns -1 if the most recently computed fp16 gradients overflowed (that is, if ``self.overflow`` is ``True``).
-        """
-        if not self.overflow:
-            fp32_params = []
-            for param_group in self.optimizer.param_groups:
-                for param in param_group['params']:
-                    fp32_params.append(param)
-            return self.clip_grad_norm(fp32_params, max_norm, norm_type)
-        else:
-            return -1
-
-    def state_dict(self):
-        """
-        Returns a dict containing the current state of this :class:`FP16_Optimizer` instance.
-        This dict contains attributes of :class:`FP16_Optimizer`, as well as the state_dict
-        of the contained Pytorch optimizer.
-        Example::
-
-            checkpoint = {}
-            checkpoint['model'] = model.state_dict()
-            checkpoint['optimizer'] = optimizer.state_dict()
-            torch.save(checkpoint, "saved.pth")
-        """
-        state_dict = {}
-        state_dict['loss_scaler'] = self.loss_scaler
-        state_dict['dynamic_loss_scale'] = self.dynamic_loss_scale
-        state_dict['overflow'] = self.overflow
-        state_dict['first_closure_call_this_step'] = self.first_closure_call_this_step
-        state_dict['optimizer_state_dict'] = self.optimizer.state_dict()
-        state_dict['fp32_from_fp16'] = self.fp32_from_fp16_groups
-        return state_dict
-
-    def load_state_dict(self, state_dict):
-        """
-        Loads a state_dict created by an earlier call to state_dict(). 
-        If ``fp16_optimizer_instance`` was constructed from some ``init_optimizer``, 
-        whose parameters in turn came from ``model``, it is expected that the user 
-        will call ``model.load_state_dict()`` before
-        ``fp16_optimizer_instance.load_state_dict()`` is called.
-
-        Example::
-
-            model = torch.nn.Linear(D_in, D_out).cuda().half()
-            optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)
-            optimizer = FP16_Optimizer(optimizer, static_loss_scale = 128.0)
-            ...
-            checkpoint = torch.load("saved.pth")
-            model.load_state_dict(checkpoint['model'])
-            optimizer.load_state_dict(checkpoint['optimizer'])
-        """
-        # I think it should actually be ok to reload the optimizer before the model.
-        self.loss_scaler = state_dict['loss_scaler']
-        self.dynamic_loss_scale = state_dict['dynamic_loss_scale']
-        self.overflow = state_dict['overflow']
-        self.first_closure_call_this_step = state_dict['first_closure_call_this_step']
-        self.optimizer.load_state_dict(state_dict['optimizer_state_dict'])
-        # At this point, the optimizer's references to the model's fp32 parameters are up to date.
-        # The optimizer's hyperparameters and internal buffers are also up to date.  
-        # However, the fp32 master copies of the model's fp16 params stored by the optimizer are still
-        # out of date.  There are two options.  
-        # 1:  Refresh the master params from the model's fp16 params.  
-        # This requires less storage but incurs precision loss.
-        # 2:  Save and restore the fp32 master copies separately.
-        # We choose option 2.
-        # 
-        # Pytorch Optimizer.load_state_dict casts saved buffers (e.g. momentum) to the type and device 
-        # of their associated parameters, because it's possible those buffers might not exist yet in 
-        # the current optimizer instance.  In our case, as long as the current FP16_Optimizer has been 
-        # constructed in the same way as the one whose state_dict we are loading, the same master params
-        # are guaranteed to exist, so we can just copy_() from the saved master params.
-        for current_group, saved_group in zip(self.fp32_from_fp16_groups, state_dict['fp32_from_fp16']):
-            for current, saved in zip(current_group, saved_group):
-                current.data.copy_(saved.data)
-
-    def step(self, closure=None): # could add clip option.
-        """
-        If no closure is supplied, :attr:`step` should be called after 
-        ``fp16_optimizer_obj.backward(loss)``.
-        :attr:`step` updates the fp32 master copy of parameters using the optimizer supplied to
-        :class:`FP16_Optimizer`'s constructor, then copies the updated fp32 params into the fp16 params
-        originally referenced by :class:`FP16_Optimizer`'s constructor, so the user may immediately run
-        another forward pass using their model.
-
-        If a closure is supplied, :attr:`step` may be called without a prior call to 
-        :attr:`backward(loss)`.
-        This control flow is identical to `ordinary Pytorch optimizer use`_ with closures.
-        However, the user should take care that any ``loss.backward()`` call within the closure
-        has been replaced by ``fp16_optimizer_obj.backward(loss)``.
-
-        Args:
-           closure (optional):  Closure that will be supplied to the underlying optimizer originally passed to :class:`FP16_Optimizer`'s constructor.  closure should call :attr:`zero_grad()` on the :class:`FP16_Optimizer` object, compute the loss, call :attr:`backward(loss)`, and return the loss.
-
-        Example with closure::
-
-            # optimizer is assumed to be an FP16_Optimizer object, previously constructed from an 
-            # existing pytorch optimizer.
-            for input, target in dataset:
-                def closure():
-                    optimizer.zero_grad()
-                    output = model(input)
-                    loss = loss_fn(output, target)
-                    # loss.backward() becomes:
-                    optimizer.backward(loss)
-                    return loss
-                optimizer.step(closure)
-
-        .. warning::
-            Currently, calling :attr:`step` with a closure is not compatible with dynamic loss scaling.
-
-        .. _`ordinary Pytorch optimizer use`:
-            http://pytorch.org/docs/master/optim.html#optimizer-step-closure
-        """
-
-        scale = self.loss_scaler.loss_scale()
-        # To consider:  Should this be in step(), or update_master_grads?  It works either way,
-        # but I should make it consistent with the Amp control flow, which updates the scale
-        # during backward context manager exit.
-        # self._update_scale(self.overflow)
-
-        if self.overflow:
-            # Using _amp_state.maybe_print instead of self.print here is intentional.
-            maybe_print("Gradient overflow.  Skipping step, reducing " +
-                "loss scale to {}".format(self.loss_scaler.loss_scale()))
-            return
-        
-        if closure is not None:
-            retval = self._step_with_closure(closure)
-        else:
-            # torch.cuda.nvtx.range_push("pytorch optimizer step")
-            retval = self.optimizer.step()
-            # torch.cuda.nvtx.range_pop()
-
-        self._master_params_to_model_params()
-
-        return retval
-
-    def _step_with_closure(self, closure):
-        def wrapped_closure():
-            # helpful for debugging
-            # print("Calling wrapped_closure, first_closure_call_this_step = {}"
-            #       .format(self.first_closure_call_this_step))
-            if self.first_closure_call_this_step:
-                # We expect that the fp16 params are initially fresh on entering self.step(),
-                # so _master_params_to_model_params() is unnecessary the first time wrapped_closure()
-                # is called within self.optimizer.step().
-                self.first_closure_call_this_step = False
-            else:
-                # If self.optimizer.step() internally calls wrapped_closure more than once,
-                # it may update the fp32 params after each call.  However, self.optimizer 
-                # doesn't know about the fp16 params at all.  If the fp32 params get updated,
-                # we can't rely on self.optimizer to refresh the fp16 params.  We need
-                # to handle that manually:
-                self._master_params_to_model_params()
-            # Our API expects the user to give us ownership of the backward() call by
-            # replacing all calls to loss.backward() with optimizer.backward(loss).
-            # This requirement holds whether or not the call to backward() is made within a closure.
-            # If the user is properly calling optimizer.backward(loss) within "closure," 
-            # calling closure() here will give the fp32 master params fresh gradients
-            # for the optimizer to play with, so all wrapped_closure needs to do is call 
-            # closure() and return the loss.
-            temp_loss = closure() 
-            while(self.overflow):
-                scale = self.loss_scaler.loss_scale()
-                # self._update_scale(self.overflow) # now done at the end of backward
-                print("OVERFLOW within closure! Skipping step, reducing loss scale to {}".format(
-                      self.loss_scaler.loss_scale()))
-                temp_loss = closure()
-            return temp_loss
-
-        retval = self.optimizer.step(wrapped_closure)
-
-        self.first_closure_call_this_step = True
-
-        return retval
-
-    def backward(self, loss, update_master_grads=True, retain_graph=False):
-        """ 
-        :attr:`backward` performs the following conceptual steps:
-
-        1. fp32_loss = loss.float() (see first Note below)
-        2. scaled_loss = fp32_loss*loss_scale
-        3. scaled_loss.backward(), which accumulates scaled gradients into the ``.grad`` attributes of the model's leaves (which may be fp16, fp32, or a mixture, depending how your model was defined).
-        4. fp16 grads are then copied to the master params' ``.grad`` attributes (see second Note), which are guaranteed to be fp32.
-        5. Finally, master grads are divided by loss_scale.
-
-        In this way, after :attr:`backward`, the master params have fresh gradients,
-        and :attr:`step` may be called.
-
-        .. note::
-            :attr:`backward` internally converts the loss to fp32 before applying the loss scale.
-            This provides some additional safety against overflow if the user has supplied an 
-            fp16 loss value.  
-            However, for maximum overflow safety, the user should
-            compute the loss criterion (MSE, cross entropy, etc) in fp32 before supplying it to 
-            :attr:`backward`.
-
-        .. warning::
-            The gradients found in a model's leaves after the call to 
-            :attr:`backward` should not be regarded as valid in general, 
-            because it's possible 
-            they have been scaled (and in the case of dynamic loss scaling, 
-            the scale factor may change over time).  
-            If the user wants to inspect gradients after a call to :attr:`backward`,  
-            only the master gradients should be regarded as valid.  These can be retrieved via
-            :attr:`inspect_master_grad_data()`.
-
-        Args:
-            loss:  The loss output by the user's model.  loss may be either float or half (but see first Note above).
-            update_master_grads (bool, optional, default=True):  Option to copy fp16 grads to fp32 grads on this call.  By setting this to False, the user can delay the copy, which is useful to eliminate redundant fp16->fp32 grad copies if :attr:`backward` is being called on multiple losses in one iteration.  If set to False, the user becomes responsible for calling :attr:`update_master_grads` before calling :attr:`step`.
-            retain_graph (bool, optional, default=False):  Forwards the usual ``retain_graph=True`` option to the internal call to ``loss.backward``.  If ``retain_graph`` is being used to accumulate gradient values from multiple backward passes before calling ``optimizer.step``, passing ``update_master_grads=False`` is also recommended (see Example below).
-
-        Example::
-
-            # Ordinary operation:
-            optimizer.backward(loss)
-
-            # Naive operation with multiple losses (technically valid, but less efficient):
-            # fp32 grads will be correct after the second call,  but 
-            # the first call incurs an unnecessary fp16->fp32 grad copy.
-            optimizer.backward(loss1)
-            optimizer.backward(loss2)
-
-            # More efficient way to handle multiple losses:
-            # The fp16->fp32 grad copy is delayed until fp16 grads from all 
-            # losses have been accumulated.
-            optimizer.backward(loss1, update_master_grads=False)
-            optimizer.backward(loss2, update_master_grads=False)
-            optimizer.update_master_grads()
-        """ 
-        # To consider:  try multiple backward passes using retain_grad=True to find 
-        # a loss scale that works.  After you find a loss scale that works, do a final dummy
-        # backward pass with retain_graph=False to tear down the graph.  Doing this would avoid 
-        # discarding the iteration,  but probably wouldn't improve overall efficiency.  
-        scaled_loss = loss.float()*self.loss_scaler.loss_scale()
-        scaled_loss.backward(retain_graph=retain_graph)
-        if update_master_grads:
-            self.update_master_grads()
-
-    def update_master_grads(self):
-        # torch.cuda.nvtx.range_push("update_master_grads")
-        """
-        Copy the ``.grad`` attribute from stored references to fp16 parameters to 
-        the ``.grad`` attribute of the fp32 master parameters that are directly 
-        updated by the optimizer.  :attr:`update_master_grads` only needs to be called if
-        ``fp16_optimizer_obj.backward`` was called with ``update_master_grads=False``.
-        """
-        # if self.dynamic_loss_scale:
-        #     self._check_overflow()
-        #     if self.overflow: return
-        # self._model_grads_to_master_grads()
-        # self._downscale_master()
-        # Use the one-shot multi-tensor apply kernel
-        self.loss_scaler.clear_overflow_state()
-        if len(self.all_fp16_params) > 0:
-            # print("Model grads before")
-            # print([param.grad.data for param in self.all_fp16_params])
-            # I'm ONLY writing this as an incremental way to make some tests pass until
-            # I can refactor the tests as well.
-            # FP16_Optimizer should not be used by anyone.
-            model_grads = []
-            master_grads = []
-            for model_param, master_param in zip(self.all_fp16_params,
-                                                 self.all_fp32_from_fp16_params):
-                if model_param.grad is not None:
-                    model_grads.append(model_param.grad)
-                    if master_param.grad is None:
-                        master_param.grad = torch.empty_like(master_param)
-                    master_grads.append(master_param.grad)
-            self.loss_scaler.unscale(
-                model_grads,
-                master_grads,
-                self.loss_scaler.loss_scale())
-            # print("Master grads after")
-            # print([param.grad.data for param in self.all_fp32_from_fp16_params])
-        if len(self.all_fp32_from_fp32_params) > 0:
-            model_grads = []
-            master_grads = []
-            for model_param, master_param in zip(self.all_fp32_from_fp32_params,
-                                                 self.all_fp32_from_fp32_params):
-                if model_param.grad is not None:
-                    model_grads.append(model_param.grad)
-                    master_grads.append(master_param.grad)
-            # print("Model grads before")
-            # print([param.grad.data for param in self.all_fp32_from_fp32_params])
-            self.loss_scaler.unscale(
-                model_grads,
-                master_grads,
-                self.loss_scaler.loss_scale())
-            # print("Master grads after")
-            # print([param.grad.data for param in self.all_fp32_from_fp32_params])
-        # quit()
-        self.overflow = self.loss_scaler.update_scale()
-        # torch.cuda.nvtx.range_pop()
-
-
-    def inspect_master_grad_data(self):
-        """
-        When running with :class:`FP16_Optimizer`, 
-        ``.grad`` attributes of a model's fp16 leaves should not be
-        regarded as truthful, because they might be scaled.  
-        After a call to :attr:`fp16_optimizer_obj.backward(loss)`, if no overflow was encountered,
-        the fp32 master params' ``.grad``
-        attributes will contain valid gradients properly divided by the loss scale.  However, 
-        because :class:`FP16_Optimizer` flattens some parameters, accessing them may be 
-        nonintuitive.  :attr:`inspect_master_grad_data`
-        allows those gradients to be viewed with shapes corresponding to their associated model leaves.
-
-        Returns:
-            List of lists (one list for each parameter group).  The list for each parameter group
-            is a list of the ``.grad.data`` attributes of the fp32 master params belonging to that group.                 
-        """
-        if self.overflow:
-            print("Warning:  calling FP16_Optimizer.inspect_master_grad_data while in an overflow state.  "
-                  "Gradients are currently invalid (may be inf, nan, or stale).  Returning None.")
-            return None
-        else:
-            # The optimizer owns only references to master params.
-            master_grads_data = []
-            for param_group in self.optimizer.param_groups:
-                master_grads_this_group = []
-                for param in param_group['params']:
-                    if param.grad is not None:
-                        master_grads_this_group.append(param.grad.data)
-                    else:
-                        master_grads_this_group.append(None)
-                master_grads_data.append(master_grads_this_group)
-            return master_grads_data
-
-
-    # Promote loss scale so it can be retrieved or set via "fp16_optimizer_instance.loss_scale"
-    def _get_loss_scale(self):
-        return self.loss_scaler.loss_scale()
-
-    def _set_loss_scale(self, value):
-        self.loss_scaler._loss_scale = value
-
-    loss_scale = property(_get_loss_scale, _set_loss_scale)
-
-    # Promote state so it can be retrieved or set via "fp16_optimizer_instance.state"
-    def _get_state(self):
-        return self.optimizer.state
-
-    def _set_state(self, value):
-        self.optimizer.state = value
-
-    state = property(_get_state, _set_state)
-
-    # Promote param_groups so it can be retrieved or set via "fp16_optimizer_instance.param_groups"
-    # (for example, to adjust the learning rate)
-    def _get_param_groups(self):
-        return self.optimizer.param_groups
-
-    def _set_param_groups(self, value):
-        self.optimizer.param_groups = value
-
-    param_groups = property(_get_param_groups, _set_param_groups)
-
+import torch
+from torch import nn
+from torch.autograd import Variable
+from torch.nn.parameter import Parameter
+from torch._utils import _flatten_dense_tensors, _unflatten_dense_tensors
+
+from ..amp._amp_state import _amp_state, maybe_print
+from ..amp.scaler import LossScaler
+from ..multi_tensor_apply import multi_tensor_applier
+from .fp16util import model_grads_to_master_grads, master_params_to_model_params, clip_grad_norm
+
+# TODO:  Update overflow check + downscale to use Carl's fused kernel.
+class FP16_Optimizer(object):
+    def __init__(self, 
+                 init_optimizer, 
+                 static_loss_scale=1.0, 
+                 dynamic_loss_scale=False,
+                 dynamic_loss_args=None,
+                 verbose=True):
+        print("Warning:  FP16_Optimizer is deprecated and dangerous, and will be deleted soon.  "
+              "If it still works, you're probably getting lucky.  "
+              "For mixed precision, use the documented API https://nvidia.github.io/apex/amp.html, with opt_level=O1.")
+
+        if not torch.cuda.is_available:
+            raise SystemError("Cannot use fp16 without CUDA.")
+
+        self.verbose = verbose
+
+        self.optimizer = init_optimizer
+        # init_state_dict sets up an alternative way to cast per-param state tensors.
+        # Stashing here in case https://github.com/pytorch/pytorch/issues/7733 makes it necessary.
+        # init_state_dict = init_optimizer.state_dict()
+
+        self.fp16_groups = []
+        self.fp32_from_fp16_groups = []
+        self.fp32_from_fp32_groups = []
+        for i, param_group in enumerate(self.optimizer.param_groups):
+            self.maybe_print("FP16_Optimizer processing param group {}:".format(i))
+            fp16_params_this_group = []
+            fp32_params_this_group = []
+            fp32_from_fp16_params_this_group = []
+            for i, param in enumerate(param_group['params']):
+                if param.requires_grad:
+                    if param.type() == 'torch.cuda.HalfTensor':
+                        self.maybe_print("FP16_Optimizer received torch.cuda.HalfTensor with {}"
+                                         .format(param.size()))
+                        fp16_params_this_group.append(param)
+                        master_param = param.detach().clone().float()
+                        master_param.requires_grad = True
+                        param_group['params'][i] = master_param
+                        fp32_from_fp16_params_this_group.append(master_param)
+                        # Reset existing state dict key to the new master param.
+                        # We still need to recast per-param state tensors, if any, to FP32.
+                        if param in self.optimizer.state:
+                           self.optimizer.state[master_param] = self.optimizer.state.pop(param) 
+                    elif param.type() == 'torch.cuda.FloatTensor':
+                        self.maybe_print("FP16_Optimizer received torch.cuda.FloatTensor with {}"
+                                         .format(param.size()))
+                        fp32_params_this_group.append(param)
+                        param_group['params'][i] = param
+                    else:
+                        raise TypeError("Wrapped parameters must be either "
+                                        "torch.cuda.FloatTensor or torch.cuda.HalfTensor. "  
+                                        "Received {}".format(param.type()))
+            
+            self.fp16_groups.append(fp16_params_this_group)
+            self.fp32_from_fp16_groups.append(fp32_from_fp16_params_this_group)
+            self.fp32_from_fp32_groups.append(fp32_params_this_group)
+
+        self.all_fp16_params = []
+        for group in self.fp16_groups:
+            self.all_fp16_params += group
+
+        self.all_fp32_from_fp16_params = []
+        for group in self.fp32_from_fp16_groups:
+            self.all_fp32_from_fp16_params += group
+
+        self.all_fp32_from_fp32_params = []
+        for group in self.fp32_from_fp32_groups:
+            self.all_fp32_from_fp32_params += group
+
+        # Leverage state_dict() and load_state_dict() to recast preexisting per-param state tensors
+        self.optimizer.load_state_dict(self.optimizer.state_dict())
+        # alternative way to cast per-param state tensors:
+        # self.optimizer.load_state_dict(init_state_dict)
+
+        if dynamic_loss_scale:
+            self.dynamic_loss_scale = True
+            if dynamic_loss_args is not None:
+                self.loss_scaler = LossScaler("dynamic", **dynamic_loss_args)
+            else:
+                self.loss_scaler = LossScaler("dynamic")
+        else:
+            self.dynamic_loss_scale = False
+            self.loss_scaler = LossScaler(static_loss_scale)
+
+        self.overflow = False
+        self.first_closure_call_this_step = True
+
+        self.clip_grad_norm = clip_grad_norm
+
+        # TODO:  Centralize exposure and import error checking for the C backend.
+        if multi_tensor_applier.available:
+            import amp_C
+            self.multi_tensor_scale = amp_C.multi_tensor_scale
+            self._dummy_overflow_buf = torch.cuda.IntTensor([0]);
+
+    # Having self.maybe_print distinct from _amp_state.maybe_print is another artifact
+    # of having to support FP16_Optimizer separately, for the time being.
+    def maybe_print(self, msg):
+        if self.verbose:
+            print(msg)
+            
+    def __getstate__(self):
+        raise RuntimeError("FP16_Optimizer should be serialized using state_dict().")
+
+    def __setstate__(self, state):
+        raise RuntimeError("FP16_Optimizer should be deserialized using load_state_dict().")
+
+    def zero_grad(self, set_grads_to_None=False):
+        """
+        Zero fp32 and fp16 parameter grads.
+        """
+        # In principle, only the .grad attributes of the model params need to be zeroed,
+        # because gradients are copied into the FP32 master params.  However, we zero
+        # all gradients owned by the optimizer, just to be safe:
+        for group in self.optimizer.param_groups:
+             for p in group['params']:
+                 if set_grads_to_None:
+                     p.grad = None
+                 else:
+                     if p.grad is not None:
+                         p.grad.detach_()
+                         p.grad.zero_()
+
+        # Zero fp16 gradients owned by the model:
+        for fp16_group in self.fp16_groups:
+            for param in fp16_group:
+                if set_grads_to_None:
+                    param.grad = None
+                else:
+                    if param.grad is not None:
+                        param.grad.detach_() # as in torch.optim.optimizer.zero_grad()
+                        param.grad.zero_()
+
+    # Should not be used anymore.
+    # def _check_overflow(self):
+    #     params = []
+    #     for group in self.fp16_groups:
+    #         for param in group:
+    #             params.append(param)
+    #     for group in self.fp32_from_fp32_groups:
+    #         for param in group:
+    #             params.append(param)
+    #     self.overflow = self.loss_scaler.has_overflow(params)
+
+    # def _update_scale(self, has_overflow=False):
+    #     self.loss_scaler.update_scale(has_overflow)
+
+    def _master_params_to_model_params(self):
+        if multi_tensor_applier.available:
+            if len(self.all_fp16_params) > 0:
+                multi_tensor_applier(
+                    self.multi_tensor_scale,
+                    self._dummy_overflow_buf,
+                    [self.all_fp32_from_fp16_params, self.all_fp16_params],
+                    1.0)
+        else:
+            for fp16_group, fp32_from_fp16_group in zip(self.fp16_groups, self.fp32_from_fp16_groups):
+                master_params_to_model_params(fp16_group, fp32_from_fp16_group)
+
+    # To consider:  Integrate distributed with this wrapper by registering a hook on each variable
+    # that does the overflow check, gradient copy + downscale, and fp32 allreduce in a different stream.
+    # def _model_grads_to_master_grads(self):
+    #     for fp16_group, fp32_from_fp16_group in zip(self.fp16_groups, self.fp32_from_fp16_groups):
+    #         model_grads_to_master_grads(fp16_group, fp32_from_fp16_group)
+
+    # def _downscale_master(self):
+    #     if self.loss_scale != 1.0:
+    #         for group in self.optimizer.param_groups:
+    #             for param in group['params']:
+    #                 if param.grad is not None:
+    #                     param.grad.data.mul_(1./self.loss_scale)
+
+    def clip_master_grads(self, max_norm, norm_type=2):
+        """
+        Clips fp32 master gradients via ``torch.nn.utils.clip_grad_norm``.
+
+        Args:
+            max_norm (float or int): max norm of the gradients
+            norm_type (float or int): type of the used p-norm. Can be ``'inf'`` for
+                infinity norm.
+
+        Returns:
+            Total norm of the current fp32 gradients (viewed as a single vector).
+
+        .. warning::
+            Returns -1 if the most recently computed fp16 gradients overflowed (that is, if ``self.overflow`` is ``True``).
+        """
+        if not self.overflow:
+            fp32_params = []
+            for param_group in self.optimizer.param_groups:
+                for param in param_group['params']:
+                    fp32_params.append(param)
+            return self.clip_grad_norm(fp32_params, max_norm, norm_type)
+        else:
+            return -1
+
+    def state_dict(self):
+        """
+        Returns a dict containing the current state of this :class:`FP16_Optimizer` instance.
+        This dict contains attributes of :class:`FP16_Optimizer`, as well as the state_dict
+        of the contained Pytorch optimizer.
+        Example::
+
+            checkpoint = {}
+            checkpoint['model'] = model.state_dict()
+            checkpoint['optimizer'] = optimizer.state_dict()
+            torch.save(checkpoint, "saved.pth")
+        """
+        state_dict = {}
+        state_dict['loss_scaler'] = self.loss_scaler
+        state_dict['dynamic_loss_scale'] = self.dynamic_loss_scale
+        state_dict['overflow'] = self.overflow
+        state_dict['first_closure_call_this_step'] = self.first_closure_call_this_step
+        state_dict['optimizer_state_dict'] = self.optimizer.state_dict()
+        state_dict['fp32_from_fp16'] = self.fp32_from_fp16_groups
+        return state_dict
+
+    def load_state_dict(self, state_dict):
+        """
+        Loads a state_dict created by an earlier call to state_dict(). 
+        If ``fp16_optimizer_instance`` was constructed from some ``init_optimizer``, 
+        whose parameters in turn came from ``model``, it is expected that the user 
+        will call ``model.load_state_dict()`` before
+        ``fp16_optimizer_instance.load_state_dict()`` is called.
+
+        Example::
+
+            model = torch.nn.Linear(D_in, D_out).cuda().half()
+            optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)
+            optimizer = FP16_Optimizer(optimizer, static_loss_scale = 128.0)
+            ...
+            checkpoint = torch.load("saved.pth")
+            model.load_state_dict(checkpoint['model'])
+            optimizer.load_state_dict(checkpoint['optimizer'])
+        """
+        # I think it should actually be ok to reload the optimizer before the model.
+        self.loss_scaler = state_dict['loss_scaler']
+        self.dynamic_loss_scale = state_dict['dynamic_loss_scale']
+        self.overflow = state_dict['overflow']
+        self.first_closure_call_this_step = state_dict['first_closure_call_this_step']
+        self.optimizer.load_state_dict(state_dict['optimizer_state_dict'])
+        # At this point, the optimizer's references to the model's fp32 parameters are up to date.
+        # The optimizer's hyperparameters and internal buffers are also up to date.  
+        # However, the fp32 master copies of the model's fp16 params stored by the optimizer are still
+        # out of date.  There are two options.  
+        # 1:  Refresh the master params from the model's fp16 params.  
+        # This requires less storage but incurs precision loss.
+        # 2:  Save and restore the fp32 master copies separately.
+        # We choose option 2.
+        # 
+        # Pytorch Optimizer.load_state_dict casts saved buffers (e.g. momentum) to the type and device 
+        # of their associated parameters, because it's possible those buffers might not exist yet in 
+        # the current optimizer instance.  In our case, as long as the current FP16_Optimizer has been 
+        # constructed in the same way as the one whose state_dict we are loading, the same master params
+        # are guaranteed to exist, so we can just copy_() from the saved master params.
+        for current_group, saved_group in zip(self.fp32_from_fp16_groups, state_dict['fp32_from_fp16']):
+            for current, saved in zip(current_group, saved_group):
+                current.data.copy_(saved.data)
+
+    def step(self, closure=None): # could add clip option.
+        """
+        If no closure is supplied, :attr:`step` should be called after 
+        ``fp16_optimizer_obj.backward(loss)``.
+        :attr:`step` updates the fp32 master copy of parameters using the optimizer supplied to
+        :class:`FP16_Optimizer`'s constructor, then copies the updated fp32 params into the fp16 params
+        originally referenced by :class:`FP16_Optimizer`'s constructor, so the user may immediately run
+        another forward pass using their model.
+
+        If a closure is supplied, :attr:`step` may be called without a prior call to 
+        :attr:`backward(loss)`.
+        This control flow is identical to `ordinary Pytorch optimizer use`_ with closures.
+        However, the user should take care that any ``loss.backward()`` call within the closure
+        has been replaced by ``fp16_optimizer_obj.backward(loss)``.
+
+        Args:
+           closure (optional):  Closure that will be supplied to the underlying optimizer originally passed to :class:`FP16_Optimizer`'s constructor.  closure should call :attr:`zero_grad()` on the :class:`FP16_Optimizer` object, compute the loss, call :attr:`backward(loss)`, and return the loss.
+
+        Example with closure::
+
+            # optimizer is assumed to be an FP16_Optimizer object, previously constructed from an 
+            # existing pytorch optimizer.
+            for input, target in dataset:
+                def closure():
+                    optimizer.zero_grad()
+                    output = model(input)
+                    loss = loss_fn(output, target)
+                    # loss.backward() becomes:
+                    optimizer.backward(loss)
+                    return loss
+                optimizer.step(closure)
+
+        .. warning::
+            Currently, calling :attr:`step` with a closure is not compatible with dynamic loss scaling.
+
+        .. _`ordinary Pytorch optimizer use`:
+            http://pytorch.org/docs/master/optim.html#optimizer-step-closure
+        """
+
+        scale = self.loss_scaler.loss_scale()
+        # To consider:  Should this be in step(), or update_master_grads?  It works either way,
+        # but I should make it consistent with the Amp control flow, which updates the scale
+        # during backward context manager exit.
+        # self._update_scale(self.overflow)
+
+        if self.overflow:
+            # Using _amp_state.maybe_print instead of self.print here is intentional.
+            maybe_print("Gradient overflow.  Skipping step, reducing " +
+                "loss scale to {}".format(self.loss_scaler.loss_scale()))
+            return
+        
+        if closure is not None:
+            retval = self._step_with_closure(closure)
+        else:
+            # torch.cuda.nvtx.range_push("pytorch optimizer step")
+            retval = self.optimizer.step()
+            # torch.cuda.nvtx.range_pop()
+
+        self._master_params_to_model_params()
+
+        return retval
+
+    def _step_with_closure(self, closure):
+        def wrapped_closure():
+            # helpful for debugging
+            # print("Calling wrapped_closure, first_closure_call_this_step = {}"
+            #       .format(self.first_closure_call_this_step))
+            if self.first_closure_call_this_step:
+                # We expect that the fp16 params are initially fresh on entering self.step(),
+                # so _master_params_to_model_params() is unnecessary the first time wrapped_closure()
+                # is called within self.optimizer.step().
+                self.first_closure_call_this_step = False
+            else:
+                # If self.optimizer.step() internally calls wrapped_closure more than once,
+                # it may update the fp32 params after each call.  However, self.optimizer 
+                # doesn't know about the fp16 params at all.  If the fp32 params get updated,
+                # we can't rely on self.optimizer to refresh the fp16 params.  We need
+                # to handle that manually:
+                self._master_params_to_model_params()
+            # Our API expects the user to give us ownership of the backward() call by
+            # replacing all calls to loss.backward() with optimizer.backward(loss).
+            # This requirement holds whether or not the call to backward() is made within a closure.
+            # If the user is properly calling optimizer.backward(loss) within "closure," 
+            # calling closure() here will give the fp32 master params fresh gradients
+            # for the optimizer to play with, so all wrapped_closure needs to do is call 
+            # closure() and return the loss.
+            temp_loss = closure() 
+            while(self.overflow):
+                scale = self.loss_scaler.loss_scale()
+                # self._update_scale(self.overflow) # now done at the end of backward
+                print("OVERFLOW within closure! Skipping step, reducing loss scale to {}".format(
+                      self.loss_scaler.loss_scale()))
+                temp_loss = closure()
+            return temp_loss
+
+        retval = self.optimizer.step(wrapped_closure)
+
+        self.first_closure_call_this_step = True
+
+        return retval
+
+    def backward(self, loss, update_master_grads=True, retain_graph=False):
+        """ 
+        :attr:`backward` performs the following conceptual steps:
+
+        1. fp32_loss = loss.float() (see first Note below)
+        2. scaled_loss = fp32_loss*loss_scale
+        3. scaled_loss.backward(), which accumulates scaled gradients into the ``.grad`` attributes of the model's leaves (which may be fp16, fp32, or a mixture, depending how your model was defined).
+        4. fp16 grads are then copied to the master params' ``.grad`` attributes (see second Note), which are guaranteed to be fp32.
+        5. Finally, master grads are divided by loss_scale.
+
+        In this way, after :attr:`backward`, the master params have fresh gradients,
+        and :attr:`step` may be called.
+
+        .. note::
+            :attr:`backward` internally converts the loss to fp32 before applying the loss scale.
+            This provides some additional safety against overflow if the user has supplied an 
+            fp16 loss value.  
+            However, for maximum overflow safety, the user should
+            compute the loss criterion (MSE, cross entropy, etc) in fp32 before supplying it to 
+            :attr:`backward`.
+
+        .. warning::
+            The gradients found in a model's leaves after the call to 
+            :attr:`backward` should not be regarded as valid in general, 
+            because it's possible 
+            they have been scaled (and in the case of dynamic loss scaling, 
+            the scale factor may change over time).  
+            If the user wants to inspect gradients after a call to :attr:`backward`,  
+            only the master gradients should be regarded as valid.  These can be retrieved via
+            :attr:`inspect_master_grad_data()`.
+
+        Args:
+            loss:  The loss output by the user's model.  loss may be either float or half (but see first Note above).
+            update_master_grads (bool, optional, default=True):  Option to copy fp16 grads to fp32 grads on this call.  By setting this to False, the user can delay the copy, which is useful to eliminate redundant fp16->fp32 grad copies if :attr:`backward` is being called on multiple losses in one iteration.  If set to False, the user becomes responsible for calling :attr:`update_master_grads` before calling :attr:`step`.
+            retain_graph (bool, optional, default=False):  Forwards the usual ``retain_graph=True`` option to the internal call to ``loss.backward``.  If ``retain_graph`` is being used to accumulate gradient values from multiple backward passes before calling ``optimizer.step``, passing ``update_master_grads=False`` is also recommended (see Example below).
+
+        Example::
+
+            # Ordinary operation:
+            optimizer.backward(loss)
+
+            # Naive operation with multiple losses (technically valid, but less efficient):
+            # fp32 grads will be correct after the second call,  but 
+            # the first call incurs an unnecessary fp16->fp32 grad copy.
+            optimizer.backward(loss1)
+            optimizer.backward(loss2)
+
+            # More efficient way to handle multiple losses:
+            # The fp16->fp32 grad copy is delayed until fp16 grads from all 
+            # losses have been accumulated.
+            optimizer.backward(loss1, update_master_grads=False)
+            optimizer.backward(loss2, update_master_grads=False)
+            optimizer.update_master_grads()
+        """ 
+        # To consider:  try multiple backward passes using retain_grad=True to find 
+        # a loss scale that works.  After you find a loss scale that works, do a final dummy
+        # backward pass with retain_graph=False to tear down the graph.  Doing this would avoid 
+        # discarding the iteration,  but probably wouldn't improve overall efficiency.  
+        scaled_loss = loss.float()*self.loss_scaler.loss_scale()
+        scaled_loss.backward(retain_graph=retain_graph)
+        if update_master_grads:
+            self.update_master_grads()
+
+    def update_master_grads(self):
+        # torch.cuda.nvtx.range_push("update_master_grads")
+        """
+        Copy the ``.grad`` attribute from stored references to fp16 parameters to 
+        the ``.grad`` attribute of the fp32 master parameters that are directly 
+        updated by the optimizer.  :attr:`update_master_grads` only needs to be called if
+        ``fp16_optimizer_obj.backward`` was called with ``update_master_grads=False``.
+        """
+        # if self.dynamic_loss_scale:
+        #     self._check_overflow()
+        #     if self.overflow: return
+        # self._model_grads_to_master_grads()
+        # self._downscale_master()
+        # Use the one-shot multi-tensor apply kernel
+        self.loss_scaler.clear_overflow_state()
+        if len(self.all_fp16_params) > 0:
+            # print("Model grads before")
+            # print([param.grad.data for param in self.all_fp16_params])
+            # I'm ONLY writing this as an incremental way to make some tests pass until
+            # I can refactor the tests as well.
+            # FP16_Optimizer should not be used by anyone.
+            model_grads = []
+            master_grads = []
+            for model_param, master_param in zip(self.all_fp16_params,
+                                                 self.all_fp32_from_fp16_params):
+                if model_param.grad is not None:
+                    model_grads.append(model_param.grad)
+                    if master_param.grad is None:
+                        master_param.grad = torch.empty_like(master_param)
+                    master_grads.append(master_param.grad)
+            self.loss_scaler.unscale(
+                model_grads,
+                master_grads,
+                self.loss_scaler.loss_scale())
+            # print("Master grads after")
+            # print([param.grad.data for param in self.all_fp32_from_fp16_params])
+        if len(self.all_fp32_from_fp32_params) > 0:
+            model_grads = []
+            master_grads = []
+            for model_param, master_param in zip(self.all_fp32_from_fp32_params,
+                                                 self.all_fp32_from_fp32_params):
+                if model_param.grad is not None:
+                    model_grads.append(model_param.grad)
+                    master_grads.append(master_param.grad)
+            # print("Model grads before")
+            # print([param.grad.data for param in self.all_fp32_from_fp32_params])
+            self.loss_scaler.unscale(
+                model_grads,
+                master_grads,
+                self.loss_scaler.loss_scale())
+            # print("Master grads after")
+            # print([param.grad.data for param in self.all_fp32_from_fp32_params])
+        # quit()
+        self.overflow = self.loss_scaler.update_scale()
+        # torch.cuda.nvtx.range_pop()
+
+
+    def inspect_master_grad_data(self):
+        """
+        When running with :class:`FP16_Optimizer`, 
+        ``.grad`` attributes of a model's fp16 leaves should not be
+        regarded as truthful, because they might be scaled.  
+        After a call to :attr:`fp16_optimizer_obj.backward(loss)`, if no overflow was encountered,
+        the fp32 master params' ``.grad``
+        attributes will contain valid gradients properly divided by the loss scale.  However, 
+        because :class:`FP16_Optimizer` flattens some parameters, accessing them may be 
+        nonintuitive.  :attr:`inspect_master_grad_data`
+        allows those gradients to be viewed with shapes corresponding to their associated model leaves.
+
+        Returns:
+            List of lists (one list for each parameter group).  The list for each parameter group
+            is a list of the ``.grad.data`` attributes of the fp32 master params belonging to that group.                 
+        """
+        if self.overflow:
+            print("Warning:  calling FP16_Optimizer.inspect_master_grad_data while in an overflow state.  "
+                  "Gradients are currently invalid (may be inf, nan, or stale).  Returning None.")
+            return None
+        else:
+            # The optimizer owns only references to master params.
+            master_grads_data = []
+            for param_group in self.optimizer.param_groups:
+                master_grads_this_group = []
+                for param in param_group['params']:
+                    if param.grad is not None:
+                        master_grads_this_group.append(param.grad.data)
+                    else:
+                        master_grads_this_group.append(None)
+                master_grads_data.append(master_grads_this_group)
+            return master_grads_data
+
+
+    # Promote loss scale so it can be retrieved or set via "fp16_optimizer_instance.loss_scale"
+    def _get_loss_scale(self):
+        return self.loss_scaler.loss_scale()
+
+    def _set_loss_scale(self, value):
+        self.loss_scaler._loss_scale = value
+
+    loss_scale = property(_get_loss_scale, _set_loss_scale)
+
+    # Promote state so it can be retrieved or set via "fp16_optimizer_instance.state"
+    def _get_state(self):
+        return self.optimizer.state
+
+    def _set_state(self, value):
+        self.optimizer.state = value
+
+    state = property(_get_state, _set_state)
+
+    # Promote param_groups so it can be retrieved or set via "fp16_optimizer_instance.param_groups"
+    # (for example, to adjust the learning rate)
+    def _get_param_groups(self):
+        return self.optimizer.param_groups
+
+    def _set_param_groups(self, value):
+        self.optimizer.param_groups = value
+
+    param_groups = property(_get_param_groups, _set_param_groups)
+
diff -Nur '--exclude=.git' apex/apex/optimizers/__init__.py apex-npu/apex/optimizers/__init__.py
--- apex/apex/optimizers/__init__.py	2021-04-12 04:03:22.000000000 +0000
+++ apex-npu/apex/optimizers/__init__.py	2021-06-17 07:10:45.389712070 +0000
@@ -2,4 +2,10 @@
 from .fused_adam import FusedAdam
 from .fused_novograd import FusedNovoGrad
 from .fused_lamb import FusedLAMB
-from .fused_adagrad import FusedAdagrad
\ No newline at end of file
+from .fused_adagrad import FusedAdagrad
+from .npu_fused_sgd import NpuFusedSGD
+from .npu_fused_adam import NpuFusedAdam
+from .npu_fused_bert_adam import NpuFusedBertAdam
+from .npu_fused_adadelta import NpuFusedAdadelta
+from .npu_fused_lamb import NpuFusedLamb
+from .lamb import Lamb
\ No newline at end of file
diff -Nur '--exclude=.git' apex/apex/parallel/LARC.py apex-npu/apex/parallel/LARC.py
--- apex/apex/parallel/LARC.py	2021-04-12 04:03:22.000000000 +0000
+++ apex-npu/apex/parallel/LARC.py	2021-06-17 07:10:45.389712070 +0000
@@ -86,8 +86,16 @@
                 for p in group['params']:
                     if p.grad is None:
                         continue
-                    param_norm = torch.norm(p.data)
-                    grad_norm = torch.norm(p.grad.data)
+                    # todo:restore this modification after torch.norm support any npu format
+                    format_id = p.storage().npu_format()
+                    if format_id != 0:
+                        p_cl = p.npu_format_cast(0)
+                        p_cl_grad = p.grad.npu_format_cast(0)
+                        param_norm = torch.norm(p_cl.data)
+                        grad_norm = torch.norm(p_cl_grad.data)
+                    else:
+                        param_norm = torch.norm(p.data)
+                        grad_norm = torch.norm(p.grad.data)
 
                     if param_norm != 0 and grad_norm != 0:
                         # calculate adaptive lr + weight decay
diff -Nur '--exclude=.git' apex/apex/reparameterization/__init__.py apex-npu/apex/reparameterization/__init__.py
--- apex/apex/reparameterization/__init__.py	2021-04-12 04:03:22.000000000 +0000
+++ apex-npu/apex/reparameterization/__init__.py	2021-06-17 07:10:45.393712101 +0000
@@ -1,127 +1,127 @@
-from .weight_norm import WeightNorm
-from .reparameterization import Reparameterization
-
-def apply_weight_norm(module, name='', dim=0, hook_child=True):
-    r"""
-    Applies weight normalization to a parameter in the given module.
-    If no parameter is provided, applies weight normalization to all
-    parameters in model (except 1-d vectors and scalars).
-
-    .. math::
-         \mathbf{w} = g \dfrac{\mathbf{v}}{\|\mathbf{v}\|}
-
-    Weight normalization is a reparameterization that decouples the magnitude
-    of a weight tensor from its direction. This replaces the parameter specified
-    by `name` (e.g. "weight") with two parameters: one specifying the magnitude
-    (e.g. "weight_g") and one specifying the direction (e.g. "weight_v").
-    Weight normalization is implemented via a hook that recomputes the weight
-    tensor from the magnitude and direction before every :meth:`~Module.forward`
-    call.
-
-    By default, with `dim=0`, the norm is computed independently per output
-    channel/plane. To compute a norm over the entire weight tensor, use
-    `dim=None`.
-
-    See https://arxiv.org/abs/1602.07868
-
-    Args:
-        module (nn.Module): containing module
-        name (str, optional): name of weight parameter
-        dim (int, optional): dimension over which to compute the norm
-        hook_child (boolean, optional): adds reparameterization hook to direct parent of the 
-            parameters. If False, it's added to `module` instead. Default: True
-
-    Returns:
-        The original module with the weight norm hook
-
-    Example::
-
-        >>> m = apply_weight_norm(nn.Linear(20, 40), name='weight')
-        Linear (20 -> 40)
-        >>> m.weight_g.size()
-        torch.Size([40, 1])
-        >>> m.weight_v.size()
-        torch.Size([40, 20])
-
-    """
-    return apply_reparameterization(module, reparameterization=WeightNorm, hook_child=hook_child,
-                                    name=name, dim=dim)
-
-def remove_weight_norm(module, name='', remove_all=False):
-    """
-    Removes the weight normalization reparameterization of a parameter from a module.
-    If no parameter is supplied then all weight norm parameterizations are removed.
-    Args:
-        module (nn.Module): containing module
-        name (str, optional): name of weight parameter
-    Example:
-        >>> m = apply_weight_norm(nn.Linear(20, 40))
-        >>> remove_weight_norm(m)
-    """
-    return remove_reparameterization(module, reparameterization=WeightNorm,
-                                    name=name, remove_all=remove_all)
-
-def apply_reparameterization(module, reparameterization=None, name='', dim=0, hook_child=True):
-    """
-    Applies a given weight reparameterization (such as weight normalization) to
-    a parameter in the given module. If no parameter is given, applies the reparameterization
-    to all parameters in model (except 1-d vectors and scalars).
-
-    Args:
-        module (nn.Module): containing module
-        reparameterization (Reparameterization): reparamaterization class to apply
-        name (str, optional): name of weight parameter
-        dim (int, optional): dimension over which to perform reparameterization op
-        hook_child (boolean, optional): adds reparameterization hook to direct parent of the 
-            parameters. If False, it's added to `module` instead. Default: True
-
-    Returns:
-        The original module with the reparameterization hook
-
-    Example::
-
-        >>> m = apply_reparameterization(nn.Linear(20, 40), WeightNorm)
-        Linear (20 -> 40)
-
-    """
-    assert reparameterization is not None
-    if name != '':
-        Reparameterization.apply(module, name, dim, reparameterization, hook_child)
-    else:
-        names = list(module.state_dict().keys())
-        for name in names:
-            apply_reparameterization(module, reparameterization, name, dim, hook_child)
-    return module
-
-def remove_reparameterization(module, reparameterization=Reparameterization,
-                                name='', remove_all=False):
-    """
-    Removes the given reparameterization of a parameter from a module.
-    If no parameter is supplied then all reparameterizations are removed.
-    Args:
-        module (nn.Module): containing module
-        reparameterization (Reparameterization): reparamaterization class to apply
-        name (str, optional): name of weight parameter
-        remove_all (bool, optional): if True, remove all reparamaterizations of given type. Default: False
-    Example:
-        >>> m = apply_reparameterization(nn.Linear(20, 40),WeightNorm)
-        >>> remove_reparameterization(m)
-    """
-    if name != '' or remove_all:
-        to_remove = []
-        for k, hook in module._forward_pre_hooks.items():
-            if isinstance(hook, reparameterization) and (hook.name == name or remove_all):
-                hook.remove(module)
-                to_remove.append(k)
-        if len(to_remove) > 0:
-            for k in to_remove:
-                del module._forward_pre_hooks[k]
-            return module
-        if not remove_all:
-            raise ValueError("reparameterization of '{}' not found in {}"
-                             .format(name, module))
-    else:
-        modules = [module]+[x for x in module.modules()]
-        for m in modules:
-            remove_reparameterization(m, reparameterization=reparameterization, remove_all=True)
-        return module
+from .weight_norm import WeightNorm
+from .reparameterization import Reparameterization
+
+def apply_weight_norm(module, name='', dim=0, hook_child=True):
+    r"""
+    Applies weight normalization to a parameter in the given module.
+    If no parameter is provided, applies weight normalization to all
+    parameters in model (except 1-d vectors and scalars).
+
+    .. math::
+         \mathbf{w} = g \dfrac{\mathbf{v}}{\|\mathbf{v}\|}
+
+    Weight normalization is a reparameterization that decouples the magnitude
+    of a weight tensor from its direction. This replaces the parameter specified
+    by `name` (e.g. "weight") with two parameters: one specifying the magnitude
+    (e.g. "weight_g") and one specifying the direction (e.g. "weight_v").
+    Weight normalization is implemented via a hook that recomputes the weight
+    tensor from the magnitude and direction before every :meth:`~Module.forward`
+    call.
+
+    By default, with `dim=0`, the norm is computed independently per output
+    channel/plane. To compute a norm over the entire weight tensor, use
+    `dim=None`.
+
+    See https://arxiv.org/abs/1602.07868
+
+    Args:
+        module (nn.Module): containing module
+        name (str, optional): name of weight parameter
+        dim (int, optional): dimension over which to compute the norm
+        hook_child (boolean, optional): adds reparameterization hook to direct parent of the 
+            parameters. If False, it's added to `module` instead. Default: True
+
+    Returns:
+        The original module with the weight norm hook
+
+    Example::
+
+        >>> m = apply_weight_norm(nn.Linear(20, 40), name='weight')
+        Linear (20 -> 40)
+        >>> m.weight_g.size()
+        torch.Size([40, 1])
+        >>> m.weight_v.size()
+        torch.Size([40, 20])
+
+    """
+    return apply_reparameterization(module, reparameterization=WeightNorm, hook_child=hook_child,
+                                    name=name, dim=dim)
+
+def remove_weight_norm(module, name='', remove_all=False):
+    """
+    Removes the weight normalization reparameterization of a parameter from a module.
+    If no parameter is supplied then all weight norm parameterizations are removed.
+    Args:
+        module (nn.Module): containing module
+        name (str, optional): name of weight parameter
+    Example:
+        >>> m = apply_weight_norm(nn.Linear(20, 40))
+        >>> remove_weight_norm(m)
+    """
+    return remove_reparameterization(module, reparameterization=WeightNorm,
+                                    name=name, remove_all=remove_all)
+
+def apply_reparameterization(module, reparameterization=None, name='', dim=0, hook_child=True):
+    """
+    Applies a given weight reparameterization (such as weight normalization) to
+    a parameter in the given module. If no parameter is given, applies the reparameterization
+    to all parameters in model (except 1-d vectors and scalars).
+
+    Args:
+        module (nn.Module): containing module
+        reparameterization (Reparameterization): reparamaterization class to apply
+        name (str, optional): name of weight parameter
+        dim (int, optional): dimension over which to perform reparameterization op
+        hook_child (boolean, optional): adds reparameterization hook to direct parent of the 
+            parameters. If False, it's added to `module` instead. Default: True
+
+    Returns:
+        The original module with the reparameterization hook
+
+    Example::
+
+        >>> m = apply_reparameterization(nn.Linear(20, 40), WeightNorm)
+        Linear (20 -> 40)
+
+    """
+    assert reparameterization is not None
+    if name != '':
+        Reparameterization.apply(module, name, dim, reparameterization, hook_child)
+    else:
+        names = list(module.state_dict().keys())
+        for name in names:
+            apply_reparameterization(module, reparameterization, name, dim, hook_child)
+    return module
+
+def remove_reparameterization(module, reparameterization=Reparameterization,
+                                name='', remove_all=False):
+    """
+    Removes the given reparameterization of a parameter from a module.
+    If no parameter is supplied then all reparameterizations are removed.
+    Args:
+        module (nn.Module): containing module
+        reparameterization (Reparameterization): reparamaterization class to apply
+        name (str, optional): name of weight parameter
+        remove_all (bool, optional): if True, remove all reparamaterizations of given type. Default: False
+    Example:
+        >>> m = apply_reparameterization(nn.Linear(20, 40),WeightNorm)
+        >>> remove_reparameterization(m)
+    """
+    if name != '' or remove_all:
+        to_remove = []
+        for k, hook in module._forward_pre_hooks.items():
+            if isinstance(hook, reparameterization) and (hook.name == name or remove_all):
+                hook.remove(module)
+                to_remove.append(k)
+        if len(to_remove) > 0:
+            for k in to_remove:
+                del module._forward_pre_hooks[k]
+            return module
+        if not remove_all:
+            raise ValueError("reparameterization of '{}' not found in {}"
+                             .format(name, module))
+    else:
+        modules = [module]+[x for x in module.modules()]
+        for m in modules:
+            remove_reparameterization(m, reparameterization=reparameterization, remove_all=True)
+        return module
diff -Nur '--exclude=.git' apex/apex/reparameterization/reparameterization.py apex-npu/apex/reparameterization/reparameterization.py
--- apex/apex/reparameterization/reparameterization.py	2021-04-12 04:03:22.000000000 +0000
+++ apex-npu/apex/reparameterization/reparameterization.py	2021-06-17 07:10:45.393712101 +0000
@@ -1,151 +1,151 @@
-import torch
-from torch.nn.parameter import Parameter
-import sys
-class Reparameterization(object):
-    """
-    Class interface for performing weight reparameterizations
-    Arguments:
-        name (str): name of weight parameter
-        dim (int): dimension over which to compute the norm
-        module (nn.Module): parent module to which param `name` is registered to
-        retain_forward (bool, optional): if False deletes weight on call to 
-            module.backward. Used to avoid memory leaks with DataParallel Default: True
-    Attributes:
-        reparameterization_names (list, str): contains names of all parameters 
-            needed to compute reparameterization.
-        backward_hook_key (int): torch.utils.hooks.RemovableHandle.id for hook used in module backward pass.
-    """
-
-    def __init__(self, name, dim, module, retain_forward=True):
-        self.name = name
-        self.dim = dim
-        self.evaluated = False
-        self.retain_forward = retain_forward
-        self.reparameterization_names = []
-        self.backward_hook_key = None
-        self.module = module
-
-    def compute_weight(self, module=None, name=None):
-        """
-        Computes reparameterized weight value to assign value to module attribute
-        with name `name`.
-        See WeightNorm class for example.
-        Arguments:
-            module (nn.Module): module with weight we'd like to reparameterize
-        Returns:
-            w (Tensor): Tensor object containing value of reparameterized weight
-        """
-        raise NotImplementedError
-
-    def reparameterize(self, name, weight, dim):
-        """
-        Creates Parameters to be used for reparameterization and creates names that
-        for attributes for the module these Parameters will correspond to.
-        The parameters will be registered according to the names provided.
-        See WeightNorm class for example.
-        Arguments:
-            module (nn.Module): module with weight we'd like to reparameterize
-            name (str, optional): name of weight parameter
-            dim (int, optional): dimension over which to compute parameterization
-        Returns:
-            names (list, str): names of Parameters to be used for reparameterization
-            params (list, Parameter): Parameters to be used for reparameterization
-        """
-        raise NotImplementedError
-
-    @staticmethod
-    def apply(module, name, dim, reparameterization=None, hook_child=True):
-        """
-        Applies reparametrization to module's `name` parameter and modifies instance attributes as appropriate.
-        `hook_child` adds reparameterization hook to direct parent of the parameters. If False, it's added to `module` instead.
-        """
-        if reparameterization is None:
-            reparameterization = Reparameterization
-        module2use, name2use = Reparameterization.get_module_and_name(module, name)
-        # does not work on sparse
-        if name2use is None or isinstance(module2use, (torch.nn.Embedding, torch.nn.EmbeddingBag)):
-            return
-
-        if hook_child:
-            fn = reparameterization(name2use, dim, module2use)
-        else:
-            fn = reparameterization(name, dim, module)
-
-        weight = getattr(module2use, name2use)
-        if weight.dim() <= 1:
-            return
-
-        # remove weight from parameter list
-        del module2use._parameters[name2use]
-
-        # add parameters of reparameterization of parameter to module
-        names, params = fn.reparameterize(name2use, weight, dim)
-        for n, p in zip(names, params):
-            module2use.register_parameter(n, p)
-
-        # add parameters to reparameterization so they can be removed later
-        fn.reparameterization_names = names
-
-        setattr(module2use, name2use, None)
-
-        hook_module = module2use
-        if not hook_child:
-            hook_module = module
-        # recompute weight before every forward()
-        hook_module.register_forward_pre_hook(fn)
-
-        # remove weight during backward
-        handle = hook_module.register_backward_hook(fn.backward_hook)
-        # get hook key so we can delete it later
-        fn.backward_hook_key = handle.id
-
-        return fn
-
-    @staticmethod
-    def get_module_and_name(module, name):
-        """
-        recursively fetches (possible) child module and name of weight to be reparameterized
-        """
-        name2use = None
-        module2use = None
-        names = name.split('.')
-        if len(names) == 1 and names[0] != '':
-            name2use = names[0]
-            module2use = module
-        elif len(names) > 1:
-            module2use = module
-            name2use = names[0]
-            for i in range(len(names)-1):
-                module2use = getattr(module2use, name2use)
-                name2use = names[i+1]
-        return module2use, name2use
-
-    def get_params(self, module):
-        """gets params of reparameterization based on known attribute names"""
-        return [getattr(module, n) for n in self.reparameterization_names]
-
-    def remove(self, module):
-        """removes reparameterization and backward hook (does not remove forward hook)"""
-        module2use, name2use = Reparameterization.get_module_and_name(module, self.name)
-        for p in self.get_params(module2use):
-            p.requires_grad = False
-        weight = self.compute_weight(module2use, name2use)
-        delattr(module2use, name2use)
-        for n in self.reparameterization_names:
-            del module2use._parameters[n]
-        module2use.register_parameter(name2use, Parameter(weight.data))
-        del module._backward_hooks[self.backward_hook_key]
-
-    def __call__(self, module, inputs):
-        """callable hook for forward pass"""
-        module2use, name2use = Reparameterization.get_module_and_name(module, self.name)
-        _w = getattr(module2use, name2use)
-        if not self.evaluated or _w is None:
-            setattr(module2use, name2use, self.compute_weight(module2use, name2use))
-            self.evaluated = True
-
-    def backward_hook(self, module, grad_input, grad_output):
-        """callable hook for backward pass"""
-        module2use, name2use = Reparameterization.get_module_and_name(module, self.name)
-        wn = getattr(module2use, name2use)
-        self.evaluated = False
+import torch
+from torch.nn.parameter import Parameter
+import sys
+class Reparameterization(object):
+    """
+    Class interface for performing weight reparameterizations
+    Arguments:
+        name (str): name of weight parameter
+        dim (int): dimension over which to compute the norm
+        module (nn.Module): parent module to which param `name` is registered to
+        retain_forward (bool, optional): if False deletes weight on call to 
+            module.backward. Used to avoid memory leaks with DataParallel Default: True
+    Attributes:
+        reparameterization_names (list, str): contains names of all parameters 
+            needed to compute reparameterization.
+        backward_hook_key (int): torch.utils.hooks.RemovableHandle.id for hook used in module backward pass.
+    """
+
+    def __init__(self, name, dim, module, retain_forward=True):
+        self.name = name
+        self.dim = dim
+        self.evaluated = False
+        self.retain_forward = retain_forward
+        self.reparameterization_names = []
+        self.backward_hook_key = None
+        self.module = module
+
+    def compute_weight(self, module=None, name=None):
+        """
+        Computes reparameterized weight value to assign value to module attribute
+        with name `name`.
+        See WeightNorm class for example.
+        Arguments:
+            module (nn.Module): module with weight we'd like to reparameterize
+        Returns:
+            w (Tensor): Tensor object containing value of reparameterized weight
+        """
+        raise NotImplementedError
+
+    def reparameterize(self, name, weight, dim):
+        """
+        Creates Parameters to be used for reparameterization and creates names that
+        for attributes for the module these Parameters will correspond to.
+        The parameters will be registered according to the names provided.
+        See WeightNorm class for example.
+        Arguments:
+            module (nn.Module): module with weight we'd like to reparameterize
+            name (str, optional): name of weight parameter
+            dim (int, optional): dimension over which to compute parameterization
+        Returns:
+            names (list, str): names of Parameters to be used for reparameterization
+            params (list, Parameter): Parameters to be used for reparameterization
+        """
+        raise NotImplementedError
+
+    @staticmethod
+    def apply(module, name, dim, reparameterization=None, hook_child=True):
+        """
+        Applies reparametrization to module's `name` parameter and modifies instance attributes as appropriate.
+        `hook_child` adds reparameterization hook to direct parent of the parameters. If False, it's added to `module` instead.
+        """
+        if reparameterization is None:
+            reparameterization = Reparameterization
+        module2use, name2use = Reparameterization.get_module_and_name(module, name)
+        # does not work on sparse
+        if name2use is None or isinstance(module2use, (torch.nn.Embedding, torch.nn.EmbeddingBag)):
+            return
+
+        if hook_child:
+            fn = reparameterization(name2use, dim, module2use)
+        else:
+            fn = reparameterization(name, dim, module)
+
+        weight = getattr(module2use, name2use)
+        if weight.dim() <= 1:
+            return
+
+        # remove weight from parameter list
+        del module2use._parameters[name2use]
+
+        # add parameters of reparameterization of parameter to module
+        names, params = fn.reparameterize(name2use, weight, dim)
+        for n, p in zip(names, params):
+            module2use.register_parameter(n, p)
+
+        # add parameters to reparameterization so they can be removed later
+        fn.reparameterization_names = names
+
+        setattr(module2use, name2use, None)
+
+        hook_module = module2use
+        if not hook_child:
+            hook_module = module
+        # recompute weight before every forward()
+        hook_module.register_forward_pre_hook(fn)
+
+        # remove weight during backward
+        handle = hook_module.register_backward_hook(fn.backward_hook)
+        # get hook key so we can delete it later
+        fn.backward_hook_key = handle.id
+
+        return fn
+
+    @staticmethod
+    def get_module_and_name(module, name):
+        """
+        recursively fetches (possible) child module and name of weight to be reparameterized
+        """
+        name2use = None
+        module2use = None
+        names = name.split('.')
+        if len(names) == 1 and names[0] != '':
+            name2use = names[0]
+            module2use = module
+        elif len(names) > 1:
+            module2use = module
+            name2use = names[0]
+            for i in range(len(names)-1):
+                module2use = getattr(module2use, name2use)
+                name2use = names[i+1]
+        return module2use, name2use
+
+    def get_params(self, module):
+        """gets params of reparameterization based on known attribute names"""
+        return [getattr(module, n) for n in self.reparameterization_names]
+
+    def remove(self, module):
+        """removes reparameterization and backward hook (does not remove forward hook)"""
+        module2use, name2use = Reparameterization.get_module_and_name(module, self.name)
+        for p in self.get_params(module2use):
+            p.requires_grad = False
+        weight = self.compute_weight(module2use, name2use)
+        delattr(module2use, name2use)
+        for n in self.reparameterization_names:
+            del module2use._parameters[n]
+        module2use.register_parameter(name2use, Parameter(weight.data))
+        del module._backward_hooks[self.backward_hook_key]
+
+    def __call__(self, module, inputs):
+        """callable hook for forward pass"""
+        module2use, name2use = Reparameterization.get_module_and_name(module, self.name)
+        _w = getattr(module2use, name2use)
+        if not self.evaluated or _w is None:
+            setattr(module2use, name2use, self.compute_weight(module2use, name2use))
+            self.evaluated = True
+
+    def backward_hook(self, module, grad_input, grad_output):
+        """callable hook for backward pass"""
+        module2use, name2use = Reparameterization.get_module_and_name(module, self.name)
+        wn = getattr(module2use, name2use)
+        self.evaluated = False
diff -Nur '--exclude=.git' apex/apex/reparameterization/weight_norm.py apex-npu/apex/reparameterization/weight_norm.py
--- apex/apex/reparameterization/weight_norm.py	2021-04-12 04:03:22.000000000 +0000
+++ apex-npu/apex/reparameterization/weight_norm.py	2021-06-17 07:10:45.393712101 +0000
@@ -1,78 +1,78 @@
-import torch
-from torch.nn.parameter import Parameter
-from ..fp16_utils import Fused_Weight_Norm
-import time
-
-from .reparameterization import Reparameterization
-
-def _norm(p, dim):
-    """Computes the norm over all dimensions except dim"""
-    if dim is None:
-        return p.norm()
-    elif dim == 0:
-        output_size = (p.size(0),) + (1,) * (p.dim() - 1)
-        return p.contiguous().view(p.size(0), -1).norm(dim=1).view(*output_size)
-    elif dim == p.dim() - 1:
-        output_size = (1,) * (p.dim() - 1) + (p.size(-1),)
-        return p.contiguous().view(-1, p.size(-1)).norm(dim=0).view(*output_size)
-    return _norm(p.transpose(0, dim), 0).transpose(0, dim)
-
-HALF_TYPES = (torch.cuda.HalfTensor, torch.HalfTensor)
-
-class WeightNorm(Reparameterization):
-    r"""
-    Weight normalization is a reparameterization that decouples the magnitude
-    of a weight tensor from its direction. This replaces the parameter specified
-    by `name` (e.g. "weight") with two parameters: one specifying the magnitude
-    (e.g. "weight_g") and one specifying the direction (e.g. "weight_v").
-    Weight normalization is implemented via a hook that recomputes the weight
-    tensor from the magnitude and direction before every :meth:`~Module.forward`
-    call.
-
-    .. math::
-         \mathbf{w} = g \dfrac{\mathbf{v}}{\|\mathbf{v}\|}
-
-    By default, with `dim=0`, the norm is computed independently per output
-    channel/plane. To compute a norm over the entire weight tensor, use
-    `dim=None`.
-    """
-    def compute_weight(self, module=None, name=None):
-        """
-        Computes weight normalized weight value to assign value to module attribute
-        with name `name`.
-        Arguments:
-            module (nn.Module): module with weight we'd like to reparameterize
-        Returns:
-            w (Tensor): Tensor object containing value of reparameterized weight
-        """
-        if module is None:
-            module = self.module
-        if name is None:
-            name = self.name
-        module, name = Reparameterization.get_module_and_name(module, name)
-        g = getattr(module, name + '_g')
-        v = getattr(module, name + '_v')
-
-        fused_weight_norm = Fused_Weight_Norm.apply
-        v = v.contiguous()
-        w = fused_weight_norm(v, g, self.dim)
-
-        return w
-
-    def reparameterize(self, name, weight, dim):
-        """
-        Creates Parameters v and gto be used for weight normalization
-        and creates names that for attributes for the module these Parameters
-        will correspond to. The parameters will be registered according to the names
-        provided.
-        Arguments:
-            module (nn.Module): module with weight we'd like to reparameterize
-            name (str, optional): name of weight parameter
-            dim (int, optional): dimension over which to compute parameterization
-        Returns:
-            names (list, str): names of Parameters to be used for reparameterization
-            params (list, Parameter): Parameters to be used for reparameterization
-        """
-        names = [name + '_g', name + '_v']
-        params = [Parameter(_norm(weight, dim).data), Parameter(weight.data)]
-        return names, params
+import torch
+from torch.nn.parameter import Parameter
+from ..fp16_utils import Fused_Weight_Norm
+import time
+
+from .reparameterization import Reparameterization
+
+def _norm(p, dim):
+    """Computes the norm over all dimensions except dim"""
+    if dim is None:
+        return p.norm()
+    elif dim == 0:
+        output_size = (p.size(0),) + (1,) * (p.dim() - 1)
+        return p.contiguous().view(p.size(0), -1).norm(dim=1).view(*output_size)
+    elif dim == p.dim() - 1:
+        output_size = (1,) * (p.dim() - 1) + (p.size(-1),)
+        return p.contiguous().view(-1, p.size(-1)).norm(dim=0).view(*output_size)
+    return _norm(p.transpose(0, dim), 0).transpose(0, dim)
+
+HALF_TYPES = (torch.cuda.HalfTensor, torch.HalfTensor)
+
+class WeightNorm(Reparameterization):
+    r"""
+    Weight normalization is a reparameterization that decouples the magnitude
+    of a weight tensor from its direction. This replaces the parameter specified
+    by `name` (e.g. "weight") with two parameters: one specifying the magnitude
+    (e.g. "weight_g") and one specifying the direction (e.g. "weight_v").
+    Weight normalization is implemented via a hook that recomputes the weight
+    tensor from the magnitude and direction before every :meth:`~Module.forward`
+    call.
+
+    .. math::
+         \mathbf{w} = g \dfrac{\mathbf{v}}{\|\mathbf{v}\|}
+
+    By default, with `dim=0`, the norm is computed independently per output
+    channel/plane. To compute a norm over the entire weight tensor, use
+    `dim=None`.
+    """
+    def compute_weight(self, module=None, name=None):
+        """
+        Computes weight normalized weight value to assign value to module attribute
+        with name `name`.
+        Arguments:
+            module (nn.Module): module with weight we'd like to reparameterize
+        Returns:
+            w (Tensor): Tensor object containing value of reparameterized weight
+        """
+        if module is None:
+            module = self.module
+        if name is None:
+            name = self.name
+        module, name = Reparameterization.get_module_and_name(module, name)
+        g = getattr(module, name + '_g')
+        v = getattr(module, name + '_v')
+
+        fused_weight_norm = Fused_Weight_Norm.apply
+        v = v.contiguous()
+        w = fused_weight_norm(v, g, self.dim)
+
+        return w
+
+    def reparameterize(self, name, weight, dim):
+        """
+        Creates Parameters v and gto be used for weight normalization
+        and creates names that for attributes for the module these Parameters
+        will correspond to. The parameters will be registered according to the names
+        provided.
+        Arguments:
+            module (nn.Module): module with weight we'd like to reparameterize
+            name (str, optional): name of weight parameter
+            dim (int, optional): dimension over which to compute parameterization
+        Returns:
+            names (list, str): names of Parameters to be used for reparameterization
+            params (list, Parameter): Parameters to be used for reparameterization
+        """
+        names = [name + '_g', name + '_v']
+        params = [Parameter(_norm(weight, dim).data), Parameter(weight.data)]
+        return names, params
diff -Nur '--exclude=.git' apex/setup.py apex-npu/setup.py
--- apex/setup.py	2021-04-12 04:03:22.000000000 +0000
+++ apex-npu/setup.py	2021-06-17 07:10:45.397712131 +0000
@@ -1,3 +1,19 @@
+# Copyright (c) 2020, Huawei Technologies.
+# Copyright (c) 2019, NVIDIA CORPORATION.
+# All rights reserved.
+#
+# Licensed under the BSD 3-Clause License  (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+# https://opensource.org/licenses/BSD-3-Clause
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
 import torch
 from torch.utils import cpp_extension
 from setuptools import setup, find_packages
@@ -6,6 +22,7 @@
 import sys
 import warnings
 import os
+import glob
 
 # ninja build does not work unless include_dirs are abs path
 this_dir = os.path.dirname(os.path.abspath(__file__))
@@ -32,11 +49,7 @@
           'If you wish to cross-compile for a single specific architecture,\n'
           'export TORCH_CUDA_ARCH_LIST="compute capability" before running setup.py.\n')
     if os.environ.get("TORCH_CUDA_ARCH_LIST", None) is None:
-        _, bare_metal_major, _ = get_cuda_bare_metal_version(cpp_extension.CUDA_HOME)
-        if int(bare_metal_major) == 11:
-            os.environ["TORCH_CUDA_ARCH_LIST"] = "6.0;6.1;6.2;7.0;7.5;8.0"
-        else:
-            os.environ["TORCH_CUDA_ARCH_LIST"] = "6.0;6.1;6.2;7.0;7.5"
+        os.environ["TORCH_CUDA_ARCH_LIST"] = "6.0;6.1;6.2;7.0;7.5"
 
 print("\n\ntorch.__version__  = {}\n\n".format(torch.__version__))
 TORCH_MAJOR = int(torch.__version__.split('.')[0])
@@ -50,6 +63,10 @@
 ext_modules = []
 
 extras = {}
+
+secure_compile_args = ['-fPIE', '-fPIC', '-fstack-protector-all', '-Wall']
+secure_link_args = ['-Wl,-z,now', '-Wl,-z,relro', '-Wl,-z,noexecstack', '-s']
+
 if "--pyprof" in sys.argv:
     string = "\n\nPyprof has been moved to its own dedicated repository and will " + \
              "soon be removed from Apex.  Please visit\n" + \
@@ -78,7 +95,15 @@
     sys.argv.remove("--cpp_ext")
     ext_modules.append(
         CppExtension('apex_C',
-                     ['csrc/flatten_unflatten.cpp',]))
+                     ['csrc/flatten_unflatten.cpp',],
+                     extra_compile_args=secure_compile_args,
+                     extra_link_args=secure_link_args))
+
+    ext_modules.append(
+        CppExtension('change_data_ptr',
+                     ['csrc/combine_tensors/change_dataptr.cpp',],
+                     extra_compile_args=secure_compile_args,
+                     extra_link_args=secure_link_args))
 
 def get_cuda_bare_metal_version(cuda_dir):
     raw_output = subprocess.check_output([cuda_dir + "/bin/nvcc", "-V"], universal_newlines=True)
@@ -192,6 +217,21 @@
                           extra_compile_args={'cxx': ['-O3'] + version_dependent_macros,
                                               'nvcc':['-O3'] + version_dependent_macros}))
 
+if "--npu_float_status" in sys.argv:
+    from torch.utils.cpp_extension import CppExtension
+    sys.argv.remove("--npu_float_status")
+
+    from torch.utils.cpp_extension import BuildExtension
+    cmdclass['build_ext'] = BuildExtension
+
+    sources = glob.glob(os.path.join(this_dir, 'csrc/npu_float_status', '*.cpp'))
+
+    ext_modules.append(
+        CppExtension(name='npu_float_status',
+                     sources=sources,
+                     extra_compile_args=secure_compile_args,
+                     extra_link_args=secure_link_args))
+
 if "--bnp" in sys.argv:
     from torch.utils.cpp_extension import CUDAExtension
     sys.argv.remove("--bnp")
@@ -404,7 +444,7 @@
 
 setup(
     name='apex',
-    version='0.1',
+    version='0.1+ascend',
     packages=find_packages(exclude=('build',
                                     'csrc',
                                     'include',
diff -Nur '--exclude=.git' apex/tests/L0/run_amp/test_add_param_group.py apex-npu/tests/L0/run_amp/test_add_param_group.py
--- apex/tests/L0/run_amp/test_add_param_group.py	2021-04-12 04:03:22.000000000 +0000
+++ apex-npu/tests/L0/run_amp/test_add_param_group.py	2021-06-17 07:10:45.397712131 +0000
@@ -1,3 +1,19 @@
+# Copyright (c) 2020, Huawei Technologies.
+# Copyright (c) 2019, NVIDIA CORPORATION.
+# All rights reserved.
+#
+# Licensed under the BSD 3-Clause License  (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+# https://opensource.org/licenses/BSD-3-Clause
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
 import unittest
 
 import functools as ft
@@ -9,16 +25,20 @@
 from torch import nn
 import torch.nn.functional as F
 from torch.nn import Parameter
+import numpy as np
 
-from utils import common_init, HALF, FLOAT,\
-    ALWAYS_HALF, ALWAYS_FLOAT, MATCH_INPUT
+from utils import common_init
+import sys
+sys.path.append('../')
+import device
 
 class MyModel(torch.nn.Module):
     def __init__(self, unique):
         super(MyModel, self).__init__()
         self.weight0 = Parameter(unique +
-            torch.arange(2, device='cuda', dtype=torch.float32))
-        self.weight1 = Parameter(1. + unique + torch.arange(2, device='cuda', dtype=torch.float16))
+            torch.from_numpy(np.arange(2, dtype=np.float32)))
+        self.weight1 = Parameter(1. + unique +
+            torch.from_numpy(np.arange(2, dtype=np.float16)).to(device.CALCULATE_DEVICE ))
 
     @staticmethod
     def ops(input, weight0, weight1):
@@ -33,7 +53,8 @@
 
 class TestAddParamGroup(unittest.TestCase):
     def setUp(self):
-        self.x = torch.ones((2), device='cuda', dtype=torch.float32)
+        self.device = device.CALCULATE_DEVICE
+        self.x = torch.ones((2), device=self.device, dtype=torch.float32)
         common_init(self)
 
     def tearDown(self):
@@ -54,8 +75,8 @@
         for opt_level in ("O0", "O1", "O2", "O3"):
           for zero_before_add in (True, False):
             for try_accumulation in (True, False):
-              model0 = MyModel(1)
-              model1 = MyModel(2)
+              model0 = MyModel(1).to(self.device)
+              model1 = MyModel(2).to(self.device)
 
               optimizer = torch.optim.SGD([{'params' : model0.parameters(), 'lr' : 0.25}],
                                           momentum=0.125)
@@ -89,8 +110,8 @@
                                  [param.data.clone() for param in model1.parameters()]
 
               for how_to_zero in "none", "model", "optimizer":
-                  model0 = MyModel(1)
-                  model1 = MyModel(2)
+                  model0 = MyModel(1).to(self.device)
+                  model1 = MyModel(2).to(self.device)
 
                   optimizer = torch.optim.SGD([{'params' : model0.parameters(), 'lr' : 0.25}],
                                               momentum=0.125)
@@ -139,7 +160,8 @@
                                  [param.data.clone() for param in model1.parameters()]
 
                   for reference, final in zip(reference_params, final_params):
-                      self.assertTrue(torch.allclose(reference.to(final.dtype), final),
+                      final = final.to(torch.float32)
+                      self.assertTrue(torch.allclose(reference.to(final.dtype).to('cpu'), final.to('cpu')),
                                       "opt_level = {}, how_to_zero = {}, zero_before_add = {}".format(
                                       opt_level, how_to_zero, zero_before_add))
 
diff -Nur '--exclude=.git' apex/tests/L0/run_amp/test_basic_casts.py apex-npu/tests/L0/run_amp/test_basic_casts.py
--- apex/tests/L0/run_amp/test_basic_casts.py	2021-04-12 04:03:22.000000000 +0000
+++ apex-npu/tests/L0/run_amp/test_basic_casts.py	2021-06-17 07:10:45.397712131 +0000
@@ -1,3 +1,19 @@
+# Copyright (c) 2020, Huawei Technologies.
+# Copyright (c) 2019, NVIDIA CORPORATION.
+# All rights reserved.
+#
+# Licensed under the BSD 3-Clause License  (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+# https://opensource.org/licenses/BSD-3-Clause
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
 import unittest
 
 import functools as ft
@@ -7,73 +23,89 @@
 import torch
 from torch import nn
 import torch.nn.functional as F
+import numpy as np
+
+from utils import common_init, generate_data
+import utils
+
+import sys
+sys.path.append('../')
+import device
+
+npu_input_grad = None
 
-from utils import common_init, HALF, FLOAT,\
-    ALWAYS_HALF, ALWAYS_FLOAT, MATCH_INPUT
+def npu_input_grad_hook(grad):
+   global npu_input_grad
+   npu_input_grad = grad.to('cpu')
 
 def run_layer_test(test_case, fns, expected, input_shape, test_backward=True):
     for fn, typ in it.product(fns, expected.keys()):
-        x = torch.randn(input_shape, dtype=typ).requires_grad_()
+        x = generate_data(0, 10, input_shape, typ).requires_grad_()
+        x = x.to(test_case.device)
+        x.register_hook(npu_input_grad_hook)
         y = fn(x)
         test_case.assertEqual(y.type(), expected[typ])
         if test_backward:
             y.float().sum().backward()
-            test_case.assertEqual(x.grad.type(), MATCH_INPUT[typ])
+            test_case.assertEqual(npu_input_grad.type().split(".")[-1], utils.MATCH_INPUT[typ].split(".")[-1])
 
 class TestBasicCasts(unittest.TestCase):
     def setUp(self):
         self.handle = amp.init(enabled=True)
+        self.device = device.CALCULATE_DEVICE
         common_init(self)
 
     def tearDown(self):
         self.handle._deactivate()
 
     def test_linear_is_half(self):
-        m = nn.Linear(self.h, self.h)
+        m = nn.Linear(self.h, self.h).to(self.device)
         f = ft.partial(F.linear, weight=m.weight, bias=m.bias)
-        run_layer_test(self, [m, f], ALWAYS_HALF, (self.b, self.h))
+        run_layer_test(self, [m, f], utils.ALWAYS_HALF, (self.b, self.h))
 
     def test_conv2d_is_half(self):
-        m = nn.Conv2d(self.c, self.c, self.k)
+        m = nn.Conv2d(self.c, self.c, self.k).to(self.device)
         f = ft.partial(F.conv2d, weight=m.weight, bias=m.bias)
-        run_layer_test(self, [m, f], ALWAYS_HALF, (self.b, self.c, self.h, self.h))
+        run_layer_test(self, [m, f], utils.ALWAYS_HALF, (self.b, self.c, self.h, self.h))
 
     def test_softmax_is_float(self):
-        m = nn.Softmax(dim=1)
+        m = nn.Softmax(dim=1).to(self.device)
         f = ft.partial(F.softmax, dim=1)
-        run_layer_test(self, [m, f], ALWAYS_FLOAT, (self.b, self.h))
+        run_layer_test(self, [m, f], utils.ALWAYS_FLOAT, (self.b, self.h))
 
+    @unittest.skipIf(device.is_npu(),"NPU does not support group_norm in half")
     def test_group_norm_is_float(self):
-        m = nn.GroupNorm(num_groups=4, num_channels=self.c)
-        run_layer_test(self, [m], ALWAYS_FLOAT, (self.b, self.c, self.h, self.h))
+        m = nn.GroupNorm(num_groups=4, num_channels=self.c).to(self.device)
+        run_layer_test(self, [m], utils.ALWAYS_FLOAT, (self.b, self.c, self.h, self.h))
 
     def test_mse_loss_is_float(self):
         shape = (self.b, self.h)
-        target = torch.randn(shape)
-        mod = nn.MSELoss()
+        target = torch.randn(shape).to(self.device)
+        mod = nn.MSELoss().to(self.device)
         m = lambda x: mod(x, target)
         f = ft.partial(F.mse_loss, target=target)
-        run_layer_test(self, [m], ALWAYS_FLOAT, shape)
+        run_layer_test(self, [m], utils.ALWAYS_FLOAT, shape)
 
     def test_relu_is_match(self):
-        run_layer_test(self, [nn.ReLU(), F.relu], MATCH_INPUT, (self.b, self.h))
+        run_layer_test(self, [nn.ReLU(), F.relu], utils.MATCH_INPUT, (self.b, self.h))
 
     def test_batch_norm_is_match(self):
-        m = nn.BatchNorm2d(num_features=self.c)
+        m = nn.BatchNorm2d(num_features=self.c).to(self.device)
         f = ft.partial(F.batch_norm, running_mean=m.running_mean, running_var=m.running_var,
                        weight=m.weight, bias=m.bias, training=True)
-        run_layer_test(self, [m], MATCH_INPUT, (self.b, self.c, self.h, self.h))
+        run_layer_test(self, [m], utils.MATCH_INPUT, (self.b, self.c, self.h, self.h))
 
         # Test forward-only for BN inference
         m.eval()
         f = ft.partial(F.batch_norm, running_mean=m.running_mean, running_var=m.running_var,
                        weight=m.weight, bias=m.bias, training=False)
-        run_layer_test(self, [m, f], MATCH_INPUT, (self.b, self.c, self.h, self.h),
+        run_layer_test(self, [m, f], utils.MATCH_INPUT, (self.b, self.c, self.h, self.h),
                             test_backward=False)
 
 class TestBannedMethods(unittest.TestCase):
     def setUp(self):
         self.handle = amp.init(enabled=True)
+        self.device = device.CALCULATE_DEVICE
         common_init(self)
 
     def tearDown(self):
@@ -81,12 +113,12 @@
 
     def bce_common(self, assertion):
         shape = (self.b, self.h)
-        target = torch.rand(shape)
-        mod = nn.BCELoss()
+        target = torch.rand(shape).to(self.device)
+        mod = nn.BCELoss().to(self.device)
         m = lambda x: mod(x, target)
         f = ft.partial(F.binary_cross_entropy, target=target)
         for fn in [m, f]:
-            x = torch.rand(shape, dtype=torch.half)
+            x = generate_data(0, 10, shape, np.float16).to(self.device)
             assertion(fn, x)
 
     def test_bce_raises_by_default(self):
@@ -96,36 +128,37 @@
     def test_bce_is_float_with_allow_banned(self):
         self.handle._deactivate()
         self.handle = amp.init(enabled=True, allow_banned=True)
-        assertion = lambda fn, x: self.assertEqual(fn(x).type(), FLOAT)
+        assertion = lambda fn, x: self.assertEqual(fn(x).type(), utils.FLOAT)
         self.bce_common(assertion)
 
 class TestTensorCasts(unittest.TestCase):
     def setUp(self):
         self.handle = amp.init(enabled=True)
+        self.device = device.CALCULATE_DEVICE
         common_init(self)
 
     def tearDown(self):
         self.handle._deactivate()
 
     def test_matmul_method_is_half(self):
-        other = torch.randn(self.h, self.h)
+        other = torch.randn(self.h, self.h).to(self.device)
         lhs = lambda x: x.matmul(other)
         rhs = lambda x: other.matmul(x)
-        run_layer_test(self, [lhs, rhs], ALWAYS_HALF, (self.h, self.h))
+        run_layer_test(self, [lhs, rhs], utils.ALWAYS_HALF, (self.h, self.h))
 
     def test_matmul_op_is_half(self):
-        other = torch.randn(self.h, self.h)
+        other = torch.randn(self.h, self.h).to(self.device)
         lhs = lambda x: x @ other
         rhs = lambda x: other @ x
-        run_layer_test(self, [lhs, rhs], ALWAYS_HALF, (self.h, self.h))
+        run_layer_test(self, [lhs, rhs], utils.ALWAYS_HALF, (self.h, self.h))
 
     def test_pow_method_is_float(self):
         fn = lambda x: x.pow(2.)
-        run_layer_test(self, [fn], ALWAYS_FLOAT, (self.b, self.h))
+        run_layer_test(self, [fn], utils.ALWAYS_FLOAT, (self.b, self.h))
 
     def test_pow_op_is_float(self):
         fn = lambda x: x ** 2.
-        run_layer_test(self, [fn], ALWAYS_FLOAT, (self.b, self.h))
+        run_layer_test(self, [fn], utils.ALWAYS_FLOAT, (self.b, self.h))
 
     def test_cpu_is_float(self):
         fn = lambda x: x.cpu()
@@ -135,7 +168,7 @@
 
     def test_sum_is_float(self):
         fn = lambda x: x.sum()
-        run_layer_test(self, [fn], ALWAYS_FLOAT, (self.b, self.h))
+        run_layer_test(self, [fn], utils.ALWAYS_FLOAT, (self.b, self.h))
 
     # TODO: maybe more tests on disabled casting?
 
diff -Nur '--exclude=.git' apex/tests/L0/run_amp/test_cache.py apex-npu/tests/L0/run_amp/test_cache.py
--- apex/tests/L0/run_amp/test_cache.py	2021-04-12 04:03:22.000000000 +0000
+++ apex-npu/tests/L0/run_amp/test_cache.py	2021-06-17 07:10:45.397712131 +0000
@@ -1,3 +1,19 @@
+# Copyright (c) 2020, Huawei Technologies.
+# Copyright (c) 2019, NVIDIA CORPORATION.
+# All rights reserved.
+#
+# Licensed under the BSD 3-Clause License  (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+# https://opensource.org/licenses/BSD-3-Clause
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
 import unittest
 
 import functools as ft
@@ -8,9 +24,16 @@
 import torch
 from torch import nn
 import torch.nn.functional as F
+import numpy as np
+import sys
+sys.path.append('../')
+import device
+import utils
 
 from utils import common_init, HALF, FLOAT,\
-    ALWAYS_HALF, ALWAYS_FLOAT, MATCH_INPUT
+    ALWAYS_HALF, ALWAYS_FLOAT, MATCH_INPUT,\
+    generate_data
+    
 
 def get_reference_grad(i, w, ops):
     # Creating new tensors ensures, among other things, that the new tensors are not in the cache.
@@ -24,7 +47,8 @@
 class WhitelistModule(torch.nn.Module):
     def __init__(self, dtype):
         super(WhitelistModule, self).__init__()
-        self.weight = torch.nn.Parameter(torch.arange(8*8, device='cuda', dtype=dtype).view(8,8))
+        weight_parameter = torch.from_numpy(np.arange(8*8, dtype=dtype)).view(8,8).to(device.CALCULATE_DEVICE)
+        self.weight = torch.nn.Parameter(weight_parameter)
 
     @staticmethod
     def ops(input, weight):
@@ -37,7 +61,8 @@
 class BlacklistModule(torch.nn.Module):
     def __init__(self, dtype):
         super(BlacklistModule, self).__init__()
-        self.weight = torch.nn.Parameter(torch.arange(2*8, device='cuda', dtype=dtype).view(2,8))
+        weight_parameter = torch.from_numpy(np.arange(2*8, dtype=dtype)).view(2,8).to(device.CALCULATE_DEVICE)
+        self.weight = torch.nn.Parameter(weight_parameter)
 
     @staticmethod
     def ops(input, weight):
@@ -50,7 +75,8 @@
 class PromoteModule(torch.nn.Module):
     def __init__(self, dtype):
         super(PromoteModule, self).__init__()
-        self.weight = torch.nn.Parameter(torch.arange(2*8, device='cuda', dtype=dtype).view(2,8))
+        weight_parameter = torch.from_numpy(np.arange(2*8, dtype=dtype)).view(2,8).to(device.CALCULATE_DEVICE)
+        self.weight = torch.nn.Parameter(weight_parameter)
 
     @staticmethod
     def ops(input, weight):
@@ -61,14 +87,14 @@
 
 class TestCache(unittest.TestCase):
     def setUp(self):
-        self.x = torch.ones((2, 8), device='cuda', dtype=torch.float32)
+        self.x = torch.ones((2, 8), dtype=torch.float32).to(device.CALCULATE_DEVICE)
         common_init(self)
 
     def tearDown(self):
         pass
 
     def train_eval_train_test(self, module, t):
-        model = module(t).cuda()
+        model = module(t).to(device.CALCULATE_DEVICE)
         optimizer = torch.optim.SGD(model.parameters(), lr=1.0)
 
         _amp_state.allow_incoming_model_not_fp32 = True
@@ -91,10 +117,10 @@
         
             # Currently there's no difference in the allclose calls, so no need for branching,
             # but I'm keeping this in case we want different tolerances for fp16 and fp32 checks. 
-            if model.weight.grad.type() == "torch.cuda.HalfTensor":
-                self.assertTrue(torch.allclose(model.weight.grad.float(), reference_grad))
-            elif model.weight.grad.type() == "torch.cuda.FloatTensor":
-                self.assertTrue(torch.allclose(model.weight.grad.float(), reference_grad))
+            if model.weight.grad.type() == utils.HALF:
+                self.assertTrue(torch.allclose(model.weight.grad.float().to('cpu'), reference_grad.to('cpu')))
+            elif model.weight.grad.type() == utils.FLOAT:
+                self.assertTrue(torch.allclose(model.weight.grad.float().to('cpu'), reference_grad.to('cpu')))
             else:
                 raise RuntimeError("model.weight.grad.type = {}".format(model.weight.grad.type()))
 
@@ -115,22 +141,25 @@
     # I could easily have these as a set of for loops in a single test,
     # instead of going for granularity.
     def test_whitelist_module_fp16_weight(self):
-        self.train_eval_train_test(WhitelistModule, torch.float16)
+        self.train_eval_train_test(WhitelistModule, np.float16)
+
 
     def test_whitelist_module_fp32_weight(self):
-        self.train_eval_train_test(WhitelistModule, torch.float32)
+        self.train_eval_train_test(WhitelistModule, np.float32)
+
 
     def test_blacklist_module_fp16_weight(self):
-        self.train_eval_train_test(BlacklistModule, torch.float16)
+        self.train_eval_train_test(BlacklistModule, np.float16)
+
 
     def test_blacklist_module_fp32_weight(self):
-        self.train_eval_train_test(BlacklistModule, torch.float32)
+        self.train_eval_train_test(BlacklistModule, np.float32)
 
     def test_promote_module_fp16_weight(self):
-        self.train_eval_train_test(PromoteModule, torch.float16)
+        self.train_eval_train_test(PromoteModule, np.float16)
 
     def test_promote_module_fp32_weight(self):
-        self.train_eval_train_test(PromoteModule, torch.float32)
+        self.train_eval_train_test(PromoteModule, np.float32)
 
 
 if __name__ == '__main__':
diff -Nur '--exclude=.git' apex/tests/L0/run_amp/test_checkpointing.py apex-npu/tests/L0/run_amp/test_checkpointing.py
--- apex/tests/L0/run_amp/test_checkpointing.py	2021-04-12 04:03:22.000000000 +0000
+++ apex-npu/tests/L0/run_amp/test_checkpointing.py	2021-06-17 07:10:45.397712131 +0000
@@ -1,3 +1,19 @@
+# Copyright (c) 2020, Huawei Technologies.
+# Copyright (c) 2019, NVIDIA CORPORATION.
+# All rights reserved.
+#
+# Licensed under the BSD 3-Clause License  (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+# https://opensource.org/licenses/BSD-3-Clause
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
 import unittest
 
 import torch
@@ -7,9 +23,8 @@
 
 from apex import amp
 
-
 from utils import common_init, FLOAT
-
+import utils
 
 class MyModel(torch.nn.Module):
     def __init__(self):
@@ -40,7 +55,7 @@
             if 'num_batches_tracked' in key:
                 continue
             param = state_dict[key]
-            self.assertEqual(param.type(), FLOAT,
+            self.assertEqual(param.type(), utils.FLOAT,
                              'Parameter in state_dict not FLOAT')
 
     def train_step(self, model, optimizer, data, loss_ids):
diff -Nur '--exclude=.git' apex/tests/L0/run_amp/test_larc.py apex-npu/tests/L0/run_amp/test_larc.py
--- apex/tests/L0/run_amp/test_larc.py	2021-04-12 04:03:22.000000000 +0000
+++ apex-npu/tests/L0/run_amp/test_larc.py	2021-06-17 07:10:45.397712131 +0000
@@ -1,5 +1,5 @@
 import unittest
-
+import sys
 import torch
 from torch import nn
 from torch.nn import Parameter
@@ -8,12 +8,14 @@
 from apex.parallel.LARC import LARC
 from utils import common_init
 
+sys.path.append('../')
+import device
 
 class MyModel(torch.nn.Module):
     def __init__(self, unique):
         super(MyModel, self).__init__()
         self.weight0 = Parameter(
-            unique + torch.arange(2, device="cuda", dtype=torch.float32)
+            unique + torch.arange(2, device=device.CALCULATE_DEVICE, dtype=torch.float32)
         )
 
     def forward(self, input):
@@ -22,7 +24,7 @@
 
 class TestLARC(unittest.TestCase):
     def setUp(self):
-        self.x = torch.ones((2), device="cuda", dtype=torch.float32)
+        self.x = torch.ones((2), device=device.CALCULATE_DEVICE, dtype=torch.float32)
         common_init(self)
 
     def tearDown(self):
@@ -39,7 +41,7 @@
             )
 
             model, optimizer = amp.initialize(
-                model, optimizer, opt_level=opt_level, verbosity=0
+                model, optimizer, opt_level=opt_level, loss_scale=1024, verbosity=0
             )
 
             optimizer.zero_grad()
diff -Nur '--exclude=.git' apex/tests/L0/run_amp/test_promotion.py apex-npu/tests/L0/run_amp/test_promotion.py
--- apex/tests/L0/run_amp/test_promotion.py	2021-04-12 04:03:22.000000000 +0000
+++ apex-npu/tests/L0/run_amp/test_promotion.py	2021-06-17 07:10:45.397712131 +0000
@@ -1,3 +1,19 @@
+# Copyright (c) 2020, Huawei Technologies.
+# Copyright (c) 2019, NVIDIA CORPORATION.
+# All rights reserved.
+#
+# Licensed under the BSD 3-Clause License  (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+# https://opensource.org/licenses/BSD-3-Clause
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
 import unittest
 
 import itertools as it
@@ -7,11 +23,17 @@
 from torch import nn
 import torch.nn.functional as F
 
-from utils import common_init, HALF, FLOAT, DTYPES
+from utils import common_init, HALF, FLOAT, DTYPES,\
+    generate_data
+import utils
+import sys
+sys.path.append('../')
+import device
 
 class TestPromotion(unittest.TestCase):
     def setUp(self):
         self.handle = amp.init(enabled=True)
+        self.device = device.CALCULATE_DEVICE
         common_init(self)
 
     def tearDown(self):
@@ -20,12 +42,13 @@
     def run_binary_promote_test(self, fns, input_shape, x_inplace=False):
         type_pairs = it.product(DTYPES, DTYPES)
         for fn, (xtype, ytype) in it.product(fns, type_pairs):
-            x = torch.randn(input_shape, dtype=xtype).requires_grad_()
+            x = generate_data(0, 10, input_shape, xtype).requires_grad_()
             x_leaf = x
             if x_inplace:
                 # We need a non-leaf to call in place on
                 x = x.clone()
-            y = torch.randn(input_shape, dtype=ytype)
+            y = generate_data(0, 10, input_shape, dtype=ytype).to(self.device)
+            x = x.to(self.device)
             out = fn(x, y)
             if x_inplace:
                 # In place: always match xtype
@@ -33,9 +56,9 @@
             else:
                 # Out of place: match widest type
                 if xtype == torch.float or ytype == torch.float:
-                    self.assertEqual(out.type(), FLOAT)
+                    self.assertEqual(out.type(), utils.FLOAT)
                 else:
-                    self.assertEqual(out.type(), HALF)
+                    self.assertEqual(out.type(), utils.HALF)
             out.float().sum().backward()
             self.assertEqual(x_leaf.grad.dtype, xtype)
 
@@ -51,19 +74,19 @@
 
     def test_cat_matches_widest(self):
         shape = self.b
-        ys = [torch.randn(shape, dtype=torch.half) for _ in range(5)]
-        x_float = torch.randn(shape)
+        ys = [generate_data(0, 10, shape, dtype=torch.half).to(self.device) for _ in range(5)]
+        x_float = generate_data(0, 10, shape, dtype=torch.float).to(self.device)
         out = torch.cat(ys + [x_float])
-        self.assertEqual(out.type(), FLOAT)
-        x_half = torch.randn(shape, dtype=torch.half)
+        self.assertEqual(out.type(), utils.FLOAT)
+        x_half = generate_data(0, 10, shape, dtype=torch.half).to(self.device)
         out = torch.cat(ys + [x_half])
-        self.assertEqual(out.type(), HALF)
+        self.assertEqual(out.type(), utils.HALF)
 
     def test_inplace_exp_is_error_for_half(self):
-        xs = torch.randn(self.b)
+        xs = generate_data(0, 10, self.b, dtype=torch.float).to(self.device)
         xs.exp_()
-        self.assertEqual(xs.type(), FLOAT)
-        xs = torch.randn(self.b, dtype=torch.half)
+        self.assertEqual(xs.type(), utils.FLOAT)
+        xs = generate_data(0, 10, self.b, dtype=torch.half).to(self.device)
         with self.assertRaises(NotImplementedError):
             xs.exp_()
 
diff -Nur '--exclude=.git' apex/tests/L0/run_amp/test_rnn.py apex-npu/tests/L0/run_amp/test_rnn.py
--- apex/tests/L0/run_amp/test_rnn.py	2021-04-12 04:03:22.000000000 +0000
+++ apex-npu/tests/L0/run_amp/test_rnn.py	2021-06-17 07:10:45.397712131 +0000
@@ -1,3 +1,19 @@
+# Copyright (c) 2020, Huawei Technologies.
+# Copyright (c) 2019, NVIDIA CORPORATION.
+# All rights reserved.
+#
+# Licensed under the BSD 3-Clause License  (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+# https://opensource.org/licenses/BSD-3-Clause
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
 import unittest
 
 from apex import amp
@@ -5,7 +21,8 @@
 import torch
 from torch import nn
 
-from utils import common_init, HALF
+from utils import common_init
+import utils
 
 class TestRnnCells(unittest.TestCase):
     def setUp(self):
@@ -34,7 +51,7 @@
                     output = hidden
                 outputs.append(output)
             for y in outputs:
-                self.assertEqual(y.type(), HALF)
+                self.assertEqual(y.type(), utils.HALF)
             outputs[-1].float().sum().backward()
             for i, x in enumerate(xs):
                 self.assertEqual(x.grad.dtype, x.dtype)
@@ -69,7 +86,7 @@
             else:
                 hidden = hidden_fn()
             output, _ = rnn(x, hidden)
-            self.assertEqual(output.type(), HALF)
+            self.assertEqual(output.type(), utils.HALF)
             output[-1, :, :].float().sum().backward()
             self.assertEqual(x.grad.dtype, x.dtype)
 
@@ -108,7 +125,7 @@
             torch.set_default_tensor_type(torch.cuda.FloatTensor)
             hidden = torch.zeros((num_layers, self.b, self.h), dtype=typ)
             output, _ = rnn(packed_seq, hidden)
-            self.assertEqual(output.data.type(), HALF)
+            self.assertEqual(output.data.type(), utils.HALF)
             output.data.float().sum().backward()
             self.assertEqual(x.grad.dtype, x.dtype)
 
diff -Nur '--exclude=.git' apex/tests/L0/run_amp/utils.py apex-npu/tests/L0/run_amp/utils.py
--- apex/tests/L0/run_amp/utils.py	2021-04-12 04:03:22.000000000 +0000
+++ apex-npu/tests/L0/run_amp/utils.py	2021-06-17 07:10:45.397712131 +0000
@@ -1,7 +1,28 @@
+# Copyright (c) 2020, Huawei Technologies.
+# Copyright (c) 2019, NVIDIA CORPORATION.
+# All rights reserved.
+#
+# Licensed under the BSD 3-Clause License  (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+# https://opensource.org/licenses/BSD-3-Clause
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
 import torch
+import numpy as np
+
+import sys
+sys.path.append('../')
+import device
 
-HALF = 'torch.cuda.HalfTensor'
-FLOAT = 'torch.cuda.FloatTensor'
+HALF = 'torch.npu.HalfTensor'
+FLOAT = 'torch.npu.FloatTensor'
 
 DTYPES = [torch.half, torch.float]
 
@@ -18,4 +39,28 @@
     test_case.c = 16
     test_case.k = 3
     test_case.t = 10
-    torch.set_default_tensor_type(torch.cuda.FloatTensor)
+    global HALF, FLOAT, DTYPES, ALWAYS_HALF, ALWAYS_FLOAT, MATCH_INPUT
+    if device.is_npu():
+        HALF = 'torch.npu.HalfTensor'
+        FLOAT = 'torch.npu.FloatTensor'
+        torch.set_default_tensor_type(torch.FloatTensor)
+    else:
+        HALF = 'torch.cuda.HalfTensor'
+        FLOAT = 'torch.cuda.FloatTensor'
+        torch.set_default_tensor_type(torch.cuda.FloatTensor)
+
+    ALWAYS_HALF = {torch.float: HALF,
+                   torch.half: HALF}
+    ALWAYS_FLOAT = {torch.float: FLOAT,
+                    torch.half: FLOAT}
+    MATCH_INPUT = {torch.float: FLOAT,
+                   torch.half: HALF}
+
+def generate_data(min, max, shape, dtype):
+    if dtype == torch.float32:
+        dtype = np.float32
+    if dtype == torch.float16:
+        dtype = np.float16
+    input1 = np.random.uniform(min, max, shape).astype(dtype)
+    npu_input1 = torch.from_numpy(input1)
+    return npu_input1
\ No newline at end of file
diff -Nur '--exclude=.git' apex/tests/L0/run_test.py apex-npu/tests/L0/run_test.py
--- apex/tests/L0/run_test.py	2021-04-12 04:03:22.000000000 +0000
+++ apex-npu/tests/L0/run_test.py	2021-06-17 07:10:45.397712131 +0000
@@ -1,20 +1,71 @@
+# Copyright (c) 2020, Huawei Technologies.
+# Copyright (c) 2019, NVIDIA CORPORATION.
+# All rights reserved.
+#
+# Licensed under the BSD 3-Clause License  (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+# https://opensource.org/licenses/BSD-3-Clause
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
 import unittest
 import sys
-
-test_dirs = ["run_amp", "run_fp16util", "run_optimizers", "run_fused_layer_norm", "run_pyprof_nvtx", "run_pyprof_data", "run_mlp"]
+import device
+import torch
+import argparse
 
 runner = unittest.TextTestRunner(verbosity=2)
-
 errcode = 0
 
-for test_dir in test_dirs:
-    suite = unittest.TestLoader().discover(test_dir)
-
-    print("\nExecuting tests from " + test_dir)
+parser = argparse.ArgumentParser()
+parser.add_argument('--npu',
+                default=0,
+                type=int,
+                help='NPU id to use.')
+args = parser.parse_args()
+
+device.CALCULATE_DEVICE = "npu:{}".format(args.npu)
+torch.npu.set_device(device.CALCULATE_DEVICE)
+
+if device.is_npu():
+    sys.path.append('./run_amp')
+    sys.path.append('../../apex/contrib/test/')
+    from test_basic_casts import TestBannedMethods, TestTensorCasts, TestBasicCasts
+    from test_cache import TestCache
+    from test_promotion import TestPromotion
+    from test_larc import TestLARC
+    from test_combine_tensors import TestCombineTensors
+    test_dirs = ["run_amp"]
+    suite=unittest.TestSuite()
+    suite.addTest(unittest.TestLoader().loadTestsFromTestCase(TestBannedMethods))
+    suite.addTest(unittest.TestLoader().loadTestsFromTestCase(TestTensorCasts))
+    suite.addTest(unittest.TestLoader().loadTestsFromTestCase(TestBasicCasts))
+    suite.addTest(unittest.TestLoader().loadTestsFromTestCase(TestCache))
+    suite.addTest(unittest.TestLoader().loadTestsFromTestCase(TestPromotion))
+    suite.addTest(unittest.TestLoader().loadTestsFromTestCase(TestLARC))
+    suite.addTest(unittest.TestLoader().loadTestsFromTestCase(TestCombineTensors))
 
     result = runner.run(suite)
-
     if not result.wasSuccessful():
         errcode = 1
+    sys.exit(errcode)
+else:
+    test_dirs = ["run_amp", "run_fp16util", "run_optimizers", "run_fused_layer_norm", "run_pyprof_nvtx", "run_pyprof_data", "run_mlp"]
+
+    for test_dir in test_dirs:
+        suite = unittest.TestLoader().discover(test_dir)
+
+        print("\nExecuting tests from " + test_dir)
+
+        result = runner.run(suite)
+
+        if not result.wasSuccessful():
+            errcode = 1
 
-sys.exit(errcode)
+    sys.exit(errcode)
diff -Nur '--exclude=.git' apex/tests/L1/cross_product/run.sh apex-npu/tests/L1/cross_product/run.sh
--- apex/tests/L1/cross_product/run.sh	2021-04-12 04:03:22.000000000 +0000
+++ apex-npu/tests/L1/cross_product/run.sh	2021-06-17 07:10:45.401712162 +0000
@@ -3,4 +3,5 @@
 # DATADIR="/home/mcarilli/Desktop/pt18data/apex_stale/examples/imagenet/bare_metal_train_val/"
 # DATADIR="/opt/home/apex/examples/imagenet/"
 cp ../common/* .
-bash run_test.sh single_gpu $1
+# bash run_test.sh single_gpu $1
+bash run_test_npu.sh single_npu $1 $2
